[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simon-Pierre Boucher",
    "section": "",
    "text": "Greetings! I’m a doctoral candidate at the University Laval’s Department of Finance, Insurance and Real Estate, where I’m pursuing a PhD in finance. My main area of research and academic focus is on econometric methods related to time series, high-frequency data modeling, and the impact of macroeconomic announcements, as well as commodity financialization.\nTo study high-frequency financial data, I rely on the programming language R as my primary tool. At present, I’m working on my thesis, and I’m excited to be making progress on this significant research undertaking."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Simon-Pierre Boucher",
    "section": "",
    "text": "EDUCATION\n\nDoctorate in Administrative Sciences - Finance and Insurance (Ph.D.)\n\nLaval University, Quebec\n2019-present\n\n\n\nMaster of Science in Administration - Finance (M. Sc)\n\nLaval University, Quebec\n2017-2018\n\n\n\nBachelor of Business Administration - Finance (B.A.A.)\n\nLaval University, Quebec\n2013-2017\n\n\n\n\nEXPERIENCE\n\nLaval University\n\nTeaching\n\nGSF-3100 Capital market (Fall/Winter 2021, Fall 2022 and Winter 2023)\nGSF-6053 Financial Econometrics (Winter 2022)\nGSF-1500 Financial Management (Summer 2022)\n\n\n\nGraduate Teaching Assistant\n\nGSF-2101 Portfolio Management\nGSF-6025 Financial Strategies and Policies I\nGSF-6008 Corporate finance\nGSF-2102 Corporate finance\nGSF-6028 Financial Theory\n\n\n\nGraduate Research Assistant\n\n2017 - 2021\nReal estate database creation\nHedonic modeling\nLiterature review on financial problematics\nProgram econometric regression models on R, SAS, MATLAB.\n\n\n\n\n\nRESEARCH PROJECTS\n\nHas financialization changed the impact of macro announcements ?\nDo macro announcement surprises cause changes in liquidity, volatility or arbitrage opportunity in the Exchanged Traded Fund market?\n\n\n\nTECHNICAL STRENGTHS\n\nComputer Languages: SAS, R, MatLab, STATA, Python, SQL, Latex\nDatabases: COMPUSTAT, CRSP, Bloomberg, Morningstar, Thomson Reuters"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Author: Boucher, Simon-Pierre, Gagnon, Marie-Hélène and Power, Gabriel\nKeywords: commodities; futures; spillover; financialization; high-frequency; commercial; institutional; volatility; macro; announcements; surprise; events\nJEL Classification: E44, G13, G14, G15\nAbstract: We investigate, using high-frequency data, how financialization has changed the im- pact of macroeconomic announcements on commodity futures returns and volatility. We find that greater financialization dampens the impact of macroeconomic release surprises on commodity markets, as measured by price drift and volatility changes. Moreover, financial participants improve liquidity and price discovery, while reduc- ing volatility. Since traditional market participants prefer stability, our results suggest a beneficial impact of financialization. When we disaggregate the results, we find that the beneficial effects of greater financial participation are linked to money managers rather than swap dealers.\nLink\n\n\n\n\n\nAuthor: Boucher, Simon-Pierre, Gagnon, Marie-Hélène and Power, Gabriel\nKeywords: commodities; futures; spillover; high-frequency; volatility\nJEL Classification: E44, G13, G14, G15\nAbstract: In this research, we investigate the volatility dynamics in the commodity exchange- traded fund (ETF) market, a rapidly expanding sector. We utilize the Bayesian Vector Autoregression (BVAR) and Heterogeneous Autoregressive (HAR) models to gain in- sights into market behavior. Our analysis confirms the effectiveness of both models in capturing volatility dynamics. Particularly, we emphasize the influence of commod- ity prices on the volatility of commodity ETFs, underlining their interlinked nature. In the BVAR model, we observe a predominant spillover effect from the indicative net asset value (iNAV) to the ETF. This suggests that the iNAV, more than just reflecting market movements, actively influences the ETF’s volatility. The HAR model, with its time-varying betas, provides a detailed view of the changing risk exposures in this market, complementing our understanding of these dynamics. Our study contributes to existing literature by developing an intraday series for the ETF’s NAV, or iNAV. We document a bidirectional spillover between the ETF and its iNAV for ETFs backed by energy futures contracts, while a more pronounced spillover from iNAV to ETF is observed for those backed by metal futures contracts.\nLink\n\n\n\n\n\nAuthor: Boucher, Simon-Pierre, Gagnon, Marie-Hélène and Power, Gabriel\nKeywords: commodities; futures; spillover; high-frequency; volatility\nJEL Classification: E44, G13, G14, G15\nAbstract: In this study, we investigate the high-frequency impact of macroeconomic announce- ments on the volatility of key futures contracts. We use three estimators of spot volatility: intraday periodicity, kernel estimation and the intraday GARCH model. Our main objective is to determine which of these estimators best captures the mar- ket’s reaction to macroeconomic announcements. Our results show that the intra- day GARCH model stands out as the most effective in capturing market responses to macroeconomic announcements.\nLink"
  },
  {
    "objectID": "research.html#working-paper",
    "href": "research.html#working-paper",
    "title": "Research",
    "section": "",
    "text": "Author: Boucher, Simon-Pierre, Gagnon, Marie-Hélène and Power, Gabriel\nKeywords: commodities; futures; spillover; financialization; high-frequency; commercial; institutional; volatility; macro; announcements; surprise; events\nJEL Classification: E44, G13, G14, G15\nAbstract: We investigate, using high-frequency data, how financialization has changed the im- pact of macroeconomic announcements on commodity futures returns and volatility. We find that greater financialization dampens the impact of macroeconomic release surprises on commodity markets, as measured by price drift and volatility changes. Moreover, financial participants improve liquidity and price discovery, while reduc- ing volatility. Since traditional market participants prefer stability, our results suggest a beneficial impact of financialization. When we disaggregate the results, we find that the beneficial effects of greater financial participation are linked to money managers rather than swap dealers.\nLink\n\n\n\n\n\nAuthor: Boucher, Simon-Pierre, Gagnon, Marie-Hélène and Power, Gabriel\nKeywords: commodities; futures; spillover; high-frequency; volatility\nJEL Classification: E44, G13, G14, G15\nAbstract: In this research, we investigate the volatility dynamics in the commodity exchange- traded fund (ETF) market, a rapidly expanding sector. We utilize the Bayesian Vector Autoregression (BVAR) and Heterogeneous Autoregressive (HAR) models to gain in- sights into market behavior. Our analysis confirms the effectiveness of both models in capturing volatility dynamics. Particularly, we emphasize the influence of commod- ity prices on the volatility of commodity ETFs, underlining their interlinked nature. In the BVAR model, we observe a predominant spillover effect from the indicative net asset value (iNAV) to the ETF. This suggests that the iNAV, more than just reflecting market movements, actively influences the ETF’s volatility. The HAR model, with its time-varying betas, provides a detailed view of the changing risk exposures in this market, complementing our understanding of these dynamics. Our study contributes to existing literature by developing an intraday series for the ETF’s NAV, or iNAV. We document a bidirectional spillover between the ETF and its iNAV for ETFs backed by energy futures contracts, while a more pronounced spillover from iNAV to ETF is observed for those backed by metal futures contracts.\nLink\n\n\n\n\n\nAuthor: Boucher, Simon-Pierre, Gagnon, Marie-Hélène and Power, Gabriel\nKeywords: commodities; futures; spillover; high-frequency; volatility\nJEL Classification: E44, G13, G14, G15\nAbstract: In this study, we investigate the high-frequency impact of macroeconomic announce- ments on the volatility of key futures contracts. We use three estimators of spot volatility: intraday periodicity, kernel estimation and the intraday GARCH model. Our main objective is to determine which of these estimators best captures the mar- ket’s reaction to macroeconomic announcements. Our results show that the intra- day GARCH model stands out as the most effective in capturing market responses to macroeconomic announcements.\nLink"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "GSF-3100 Capital market (Fall/Winter 2021)\nThe objective of this course is to analyze capital markets, their main financial instruments and their roles in the intermediation of funds and risk; to examine the role of financial institutions and supervisory agencies, particularly in the Canadian context; analyze the term structure of interest rates and the volatility of fixed income securities, government, corporate and international bond markets; study fixed income securities with optional clauses and the main financial markets for risk intermediation classified according to the instruments traded in them, i.e., asset-backed securities, futures, options and swaps markets\n\nSlide S01\nSlide S02\nSlide S03\nSlide S04\nSlide S05\nSlide S06\nSlide S07\nSlide S08\nSlide S09A\nSlide S09B\nSlide S09C\nSlide S10A\nSlide S10B\n\nGSF-6053 Financial Econometrics (Winter 2022)\nThis course familiarizes the student with the many practical dimensions of the use of econometric methods and estimation techniques in finance. Emphasis is placed on modeling problems and financial applications. Basic econometric models and concepts related to financial markets are presented. The student is expected to have acquired a basic knowledge of statistics.\n\nSlide S02\nSlide S03\nSlide S04\nSlide S05\nSlide S06\nSlide S07\nSlide S08\nSlide S09\nSlide S10\nSlide S11\nSlide S12\n\nGSF-1500 Financial Management (Summer 2022)\nThe course aims to introduce the future administrator to the principles and techniques of modern financial management. The teaching of theoretical models is focused on the current practice of corporate financial management. It serves mainly to develop the essential tools for decision-making. The principles and tools developed in this course are critically applied to key financial decisions: the choice of investments, the evaluation of the cost of capital and the choice of permanent financing (corporate capital structure and dividend policy)."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "R script is designed to plot a budget constraint and an indifference curve",
    "section": "",
    "text": "This R script is designed to plot a budget constraint and an indifference curve for an intertemporal choice scenario within the context of financial theory. Initially, the script defines incomes for two periods (`y0` and `y1`), as well as the interest rate. It then calculates the total available amount for consumption (`W0`) by accounting for the interest rate to discount the income from year 1. This calculation ensures that the future value of money is appropriately adjusted for present value.\nSubsequently, the script generates a linear budget constraint that illustrates how consumption in year 0 and year 1 must adjust to stay within the total budget. This is depicted by plotting `x_ajuste` against `y_ajuste`, where these variables represent consumption in year 0 and year 1, respectively, adjusted across 100 points to demonstrate the trade-off between present and future consumption.\nMoreover, the script employs a logarithmic utility function to calculate and plot an indifference curve. This curve represents combinations of consumption between years 0 and 1 that provide the same level of utility. To do this, it first calculates a feasible level of utility (`utilite_realisable`) using a selected budget point (`c0_budget`, `c1_budget`) as a reference. Then, it plots a series of points (`c0_points_filtered`, `c1_points_filtered`) that achieve this fixed level of utility across different consumption mixes.\nThe final plot visually compares these two crucial concepts in financial theory: the linear budget constraint (in blue) and the curved indifference line (in red), against a backdrop of possible consumption choices in years 0 and 1. The plot effectively demonstrates the trade-offs and decision-making process involved in intertemporal consumption planning, highlighting the impact of interest rates on present and future consumption choices.\n\n# GSF-6028\n# THÉORIE FINANCIERE\n\nlibrary(ggplot2)\ny0=500\ny1=500\ntaux_interet=0.05\n\nW0 &lt;- y0 + y1 / (1 + taux_interet)\n\n\nx_ajuste &lt;- seq(0, W0, length.out = 100)  \ny_ajuste &lt;- (W0 - x_ajuste) * (1 + taux_interet)  \n\n\n\n\ndf &lt;- data.frame(x_ajuste, y_ajuste)\n\nc0_budget &lt;- W0 / 2\nc1_budget &lt;- (W0 - c0_budget) * (1 + taux_interet)\nutilite_log &lt;- function(c0, c1) {\n\n  return(log(c0) + (1/(1+taux_interet))*log(c1))\n}\n\nutilite_realisable &lt;- utilite_log(c0_budget, c1_budget)\n\ntrouver_c1_pour_utilite_fixe &lt;- function(c0, utilite_fixe) {\n  f &lt;- function(c1) utilite_log(c0, c1) - utilite_fixe\n  tryCatch({\n    return(uniroot(f, c(0.1, W0))$root)\n  }, error=function(e) {\n    return(NaN)\n  })\n}\n\nc0_points &lt;- seq(0, 1000, by = 10)\nc1_points_realisable &lt;- sapply(c0_points, function(c0) trouver_c1_pour_utilite_fixe(c0, utilite_realisable))\n\nc0_points_filtered &lt;- c0_points[!is.nan(c1_points_realisable)]\nc1_points_filtered &lt;- c1_points_realisable[!is.nan(c1_points_realisable)]\n\nplot(c0_points_filtered, c1_points_filtered, type = \"l\", col = \"red\", \n     xlab = \"Consommation année 0 (y0)\", ylab = \"Consommation année 1 (y1)\",\n     main = \"Contrainte de Richesse et Courbe d'Indifférence Ajustée\",xlim = c(0,1000),ylim = c(0,1000),lwd=3)\nlines(x_ajuste,y_ajuste,lwd=3,col=\"blue\")\ngrid()\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{boucher,\n  author = {Boucher, Simon-Pierre},\n  title = {R Script Is Designed to Plot a Budget Constraint and an\n    Indifference Curve},\n  url = {https://www.spboucher.net/posts},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoucher, Simon-Pierre. n.d. “R Script Is Designed to Plot a Budget\nConstraint and an Indifference Curve .” https://www.spboucher.net/posts."
  },
  {
    "objectID": "posts/index.html#quarto",
    "href": "posts/index.html#quarto",
    "title": "Untitled",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "posts/index.html#running-code",
    "href": "posts/index.html#running-code",
    "title": "Untitled",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nR script is designed to plot a budget constraint and an indifference curve\n\n\n\n\n\n\n\nR script\n\n\nBudget constraint\n\n\nIndifference curve\n\n\n\n\n\n\n\n\n\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nDuproprio Webscrapping - Part 3\n\n\n\n\n\n\n\nReal Estate Listing\n\n\nWeb scrapping\n\n\nWeb Harvesting\n\n\n\n\n\n\n\n\n\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nR function that computes all the main descriptive statistics\n\n\n\n\n\n\n\nDescriptive statistics\n\n\nMean\n\n\nVariance\n\n\nSkewness\n\n\nKurtosis\n\n\n\n\n\n\n\n\n\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nSimple Linear Model\n\n\n\n\n\n\n\nSimple Linear Model\n\n\nRegression\n\n\nEconometrics\n\n\n\n\n\n\n\n\n\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nLeast squares: two or more independent variables\n\n\n\n\n\n\n\nLeast squares\n\n\nIndependent variables\n\n\nEconometrics\n\n\nMatrix form\n\n\n\n\n\n\n\n\n\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nLeast squares estimation\n\n\n\n\n\n\n\nLeast squares\n\n\nMethod by summation\n\n\nEconometrics\n\n\n\n\n\n\n\n\n\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nGoodness of fit\n\n\n\n\n\n\n\nGoodness of fit\n\n\nR-SQUARED\n\n\nEconometrics\n\n\n\n\n\n\n\n\n\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nAssumption of the Classical Linear Regression Model\n\n\n\n\n\n\n\nAssumption\n\n\nLinear Regression Model\n\n\nEconometrics\n\n\n\n\n\n\n\n\n\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nDuproprio Webscrapping - Part 1\n\n\n\n\n\n\n\nReal Estate Listing\n\n\nWeb scrapping\n\n\nWeb Harvesting\n\n\n\n\n\n\n\n\n\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nDuproprio Webscrapping - Part 2\n\n\n\n\n\n\n\nReal Estate Listing\n\n\nWeb scrapping\n\n\nWeb Harvesting\n\n\n\n\n\n\n\n\n\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/index.html#r-script-is-designed-to-plot-a-budget-constraint-and-an-indifference-curve",
    "href": "posts/index.html#r-script-is-designed-to-plot-a-budget-constraint-and-an-indifference-curve",
    "title": "R script is designed to plot a budget constraint and an indifference curve",
    "section": "",
    "text": "This R script is designed to plot a budget constraint and an indifference curve for an intertemporal choice scenario within the context of financial theory. Initially, the script defines incomes for two periods (`y0` and `y1`), as well as the interest rate. It then calculates the total available amount for consumption (`W0`) by accounting for the interest rate to discount the income from year 1. This calculation ensures that the future value of money is appropriately adjusted for present value.\nSubsequently, the script generates a linear budget constraint that illustrates how consumption in year 0 and year 1 must adjust to stay within the total budget. This is depicted by plotting `x_ajuste` against `y_ajuste`, where these variables represent consumption in year 0 and year 1, respectively, adjusted across 100 points to demonstrate the trade-off between present and future consumption.\nMoreover, the script employs a logarithmic utility function to calculate and plot an indifference curve. This curve represents combinations of consumption between years 0 and 1 that provide the same level of utility. To do this, it first calculates a feasible level of utility (`utilite_realisable`) using a selected budget point (`c0_budget`, `c1_budget`) as a reference. Then, it plots a series of points (`c0_points_filtered`, `c1_points_filtered`) that achieve this fixed level of utility across different consumption mixes.\nThe final plot visually compares these two crucial concepts in financial theory: the linear budget constraint (in blue) and the curved indifference line (in red), against a backdrop of possible consumption choices in years 0 and 1. The plot effectively demonstrates the trade-offs and decision-making process involved in intertemporal consumption planning, highlighting the impact of interest rates on present and future consumption choices.\n\n# GSF-6028\n# THÉORIE FINANCIERE\n\nlibrary(ggplot2)\ny0=500\ny1=500\ntaux_interet=0.05\n\nW0 &lt;- y0 + y1 / (1 + taux_interet)\n\n\nx_ajuste &lt;- seq(0, W0, length.out = 100)  \ny_ajuste &lt;- (W0 - x_ajuste) * (1 + taux_interet)  \n\n\n\n\ndf &lt;- data.frame(x_ajuste, y_ajuste)\n\nc0_budget &lt;- W0 / 2\nc1_budget &lt;- (W0 - c0_budget) * (1 + taux_interet)\nutilite_log &lt;- function(c0, c1) {\n\n  return(log(c0) + (1/(1+taux_interet))*log(c1))\n}\n\nutilite_realisable &lt;- utilite_log(c0_budget, c1_budget)\n\ntrouver_c1_pour_utilite_fixe &lt;- function(c0, utilite_fixe) {\n  f &lt;- function(c1) utilite_log(c0, c1) - utilite_fixe\n  tryCatch({\n    return(uniroot(f, c(0.1, W0))$root)\n  }, error=function(e) {\n    return(NaN)\n  })\n}\n\nc0_points &lt;- seq(0, 1000, by = 10)\nc1_points_realisable &lt;- sapply(c0_points, function(c0) trouver_c1_pour_utilite_fixe(c0, utilite_realisable))\n\nc0_points_filtered &lt;- c0_points[!is.nan(c1_points_realisable)]\nc1_points_filtered &lt;- c1_points_realisable[!is.nan(c1_points_realisable)]\n\nplot(c0_points_filtered, c1_points_filtered, type = \"l\", col = \"red\", \n     xlab = \"Consommation année 0 (y0)\", ylab = \"Consommation année 1 (y1)\",\n     main = \"Contrainte de Richesse et Courbe d'Indifférence Ajustée\",xlim = c(0,1000),ylim = c(0,1000),lwd=3)\nlines(x_ajuste,y_ajuste,lwd=3,col=\"blue\")\ngrid()"
  },
  {
    "objectID": "posts/index2.html",
    "href": "posts/index2.html",
    "title": "R function that computes all the main descriptive statistics",
    "section": "",
    "text": "This function computeDescriptiveStats takes a numeric vector data as input and computes:\n\nMean\nMedian\nVariance\nStandard Deviation\nSkewness\nKurtosis\nMinimum\nMaximum\nRange (minimum and maximum)\nQuantiles (default quantiles include 0%, 25%, 50%, 75%, 100%)\n\nTo use this function, you need to have R installed on your system, and you should ensure that the e1071 package is installed as well. You can test this function by copying the code into an R script or R console. The set.seed(123) part ensures that the random data generated is the same every time you run the script for reproducibility purposes.\n\nLoad necessary library\n\nif (!require(e1071)) install.packages(\"e1071\")\n\nLoading required package: e1071\n\nlibrary(e1071)\n\n\n\nDefine the function\n\ncomputeDescriptiveStats &lt;- function(data) {\n  if (!is.numeric(data)) {\n    stop(\"Data must be numeric\")\n  }\n  \n  stats &lt;- list(\n    mean = mean(data),\n    median = median(data),\n    variance = var(data),\n    standard_deviation = sd(data),\n    skewness = skewness(data),\n    kurtosis = kurtosis(data),\n    minimum = min(data),\n    maximum = max(data),\n    range = range(data),\n    quantiles = quantile(data)\n  )\n  \n  return(stats)\n}\n\n\n\nTest the function with random data\n\nset.seed(123) # For reproducibility\nrandom_data &lt;- rnorm(100) # Generate 100 random numbers from a normal distribution\n\n\n\nCompute descriptive statistics\n\ndescriptive_stats &lt;- computeDescriptiveStats(random_data)\n\n\n\nPrint the results\n\nprint(descriptive_stats)\n\n$mean\n[1] 0.09040591\n\n$median\n[1] 0.06175631\n\n$variance\n[1] 0.8332328\n\n$standard_deviation\n[1] 0.9128159\n\n$skewness\n[1] 0.05959426\n\n$kurtosis\n[1] -0.217548\n\n$minimum\n[1] -2.309169\n\n$maximum\n[1] 2.187333\n\n$range\n[1] -2.309169  2.187333\n\n$quantiles\n         0%         25%         50%         75%        100% \n-2.30916888 -0.49385424  0.06175631  0.69181917  2.18733299 \n\n\n\n\n\n\nCitationBibTeX citation:@online{boucher,\n  author = {Boucher, Simon-Pierre},\n  title = {R Function That Computes All the Main Descriptive Statistics},\n  url = {https://www.spboucher.net/posts/index2.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoucher, Simon-Pierre. n.d. “R Function That Computes All the Main\nDescriptive Statistics.” https://www.spboucher.net/posts/index2.html."
  },
  {
    "objectID": "posts/index3.html",
    "href": "posts/index3.html",
    "title": "Simple Linear Model",
    "section": "",
    "text": "The intuition behind a simple linear regression model is simply to relate two variables having a theoretical economic relationship. The model will allow us to check whether this relation is also reflected empirically. Whether in economics or finance, there are several asset pricing models with a strong theoretical foundation that are not reflected empirically. The best example is the Capital Asset Pricing Model (CAPM), linking the risk premium of the market portfolio with the risk premium of an individual security. The theoretical model can therefore be represented as follows:\n\\[E(R_i)=R_f+\\beta_{i,m} [E(R_m)-R_f]\\]\nWhere :\n\n\\(\\beta_{i,m}\\) represents a measure of the systematic risk of the asset\n\\(E(R_m)\\) represents the expected profitability on the market\n\\(R_f\\) represents the risk free interest rate\n\\(E(R_i)\\) represents the expected profitability on asset \\(i\\)\n\nSince we are going to want to estimate this model empirically, we will use the realized values for the return on asset i, the market and the risk-free rate. The empirical format of the CAPM can therefore be represented as follows:\n\\[(R_i-R_f)=\\alpha+\\beta(R_m-R_f)+\\epsilon\\]\nWhere :\n\n\\((R_i-R_f)\\) represents the dependent variable (we seek to explain the variation of this variable)\n\\((R_m-R_f)\\) represents the independent variable(we will use this variable to explain the independent variable)\n\\(\\alpha\\) is the coefficient estimating the proportion of the excess return on the asset \\(i\\) which is unexplained by the systematic risk\n\\(\\beta\\) is the coefficient estimating the proportion of the excess return on the asset \\(i\\) which is explained by the systematic risk\n\\(\\epsilon\\) represents the error in our model, that is, the variations in return related to idiosyncratic risk.\n\nThis is a specific example of a simple linear regression model and this specification is by no means exhaustive. To have a uniform notation, the simple linear regression model will be represented as follows:\n\\[y_i=\\beta_0+\\beta_1x_i+e_i\\]\nWhere :\n\n\\(y_i\\) represents the dependent variable for the observation \\(i\\)\n\\(x_i\\) represents the independent variable for the observation \\(i\\)\n\\(\\beta_0\\) represents the intercept estimator of our model\n\\(\\beta_1\\) represents the slope estimator of our model\n\\(\\epsilon_i\\) represents the error made by the model\n\nThe error term \\(\\epsilon_i\\) represents the difference between the actual value of our dependent variable and its estimated value. If our model is correctly specified then the error term should not contain any structure linking it to our dependent variable. In other words, the error term \\(\\epsilon_i\\) must be independent and identically distributed random variable with mean zero and constant variance \\(\\sigma^2\\).\n\\[\\epsilon \\sim iid(0,\\sigma^2)\\]\nSince the error term is a random variable then, the dependent variable is also a random variable with the following expectation:\n\\[E(y_i)=E[\\beta_0+\\beta_1x_i+\\epsilon_i]   =E(\\beta_0+\\beta_1x_i)+E(\\epsilon_i)  =\\beta_0+\\beta_1x_i\\]\nAs for the variance of our dependent variable, we find it as follows:\n\\[Var(y_i)=Var[\\beta_0+\\beta_1x_i+\\epsilon_i]   =Var(\\beta_0+\\beta_1x_i)+Var(\\epsilon_i)  =Var(\\epsilon_i)  =\\sigma^2\\]\n\n\n\nCitationBibTeX citation:@online{boucher,\n  author = {Boucher, Simon-Pierre},\n  title = {Simple {Linear} {Model}},\n  url = {https://www.spboucher.net/posts/index3.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoucher, Simon-Pierre. n.d. “Simple Linear Model.” https://www.spboucher.net/posts/index3.html."
  },
  {
    "objectID": "posts/index4.html",
    "href": "posts/index4.html",
    "title": "Least squares: two or more independent variables",
    "section": "",
    "text": "In this chapter, we’ll apply the ordinary least-squares method again, but this time if we have more than one independent variable. Here is a regression model with several variables.\n\\[y=\\beta_0+\\beta_1 x_{1i}+\\beta_2 x_{2i}+...+\\beta_N X_{Ni}+\\epsilon\\]\nTo simplify the model, we will now use the matrix format. Here is the change regarding the variables:\n\nLet \\(Y\\) be an \\(n \\times 1\\) vector of observations on the dependent variable.\n\n\\[Y=\\begin{pmatrix} y_1\\\\  y_2\\\\  \\vdots\\\\ y_n\\\\ \\end{pmatrix}\\]\nLet \\(\\beta\\) be an \\(m \\times 1\\) vector of unknown population parameters that we want to estimate.\n\\[\\beta=\\begin{pmatrix} \\beta_1\\\\  \\beta_2\\\\  \\vdots\\\\ \\beta_n\\\\ \\end{pmatrix}\\]\nLet \\(\\epsilon\\) be an \\(n \\times 1\\) vector of disturbances or errors.\n\\[\\epsilon=\\begin{pmatrix} \\epsilon_1\\\\  \\epsilon_2\\\\  \\vdots\\\\ \\epsilon_n\\\\ \\end{pmatrix}\\]\nLet \\(X\\) be an \\(n \\times m\\) matrix where we have observations on \\(m\\) independent variables for \\(n\\) observations.\n\\[X=\\begin{bmatrix}  1&x_{11}  &x_{21}  &\\cdots   &x_{n1} \\\\   1&x_{12}  &x_{22}  & \\cdots &x_{n2} \\\\   \\vdots & \\vdots  & \\vdots  & \\ddots  & \\vdots\\\\   1&x_{1m}  & \\cdots & \\cdots &x_{nm} \\end{bmatrix}\\]\nThis allows us to rewrite the model in a simplified way\n\\[Y=X'\\beta+\\epsilon\\]\nFind the estimator of \\(\\beta\\) by minimizing the sum of squared residuals \\((\\epsilon' \\epsilon)\\)\n\\[\\epsilon'\\epsilon=(Y-X'\\beta)'(Y-X'\\beta)\\]\n\\[\\epsilon'\\epsilon= Y'Y-\\beta'X'Y-Y'X \\beta + \\beta'X'X \\beta\\]\n\\[\\epsilon'\\epsilon= Y'Y -2 \\beta' X'Y+ \\beta'X'X \\beta\\]\nThe partial derivatives with respect to \\(\\beta\\) is solved as follows :\n\\[\\frac{\\partial \\epsilon'\\epsilon}{\\partial \\beta}= 0\\]\nIt is now possible for us to find the value of \\(\\beta\\):\n\\[-2X'Y+2X'X \\beta=0\\]\n\\[X'Y=X'X \\beta\\]\nThe solution for the estimator of \\(\\beta\\):\n\\[\\hat{\\beta}=(X'X)^{-1}X'Y\\]\n\n\n\nCitationBibTeX citation:@online{boucher,\n  author = {Boucher, Simon-Pierre},\n  title = {Least Squares: Two or More Independent Variables},\n  url = {https://www.spboucher.net/posts/index4.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoucher, Simon-Pierre. n.d. “Least Squares: Two or More\nIndependent Variables.” https://www.spboucher.net/posts/index4.html."
  },
  {
    "objectID": "posts/index5.html",
    "href": "posts/index5.html",
    "title": "Least squares estimation",
    "section": "",
    "text": "In this chapter we present a way to obtain a value for our coefficient of the simple linear regression model. We start with the ordinary least squares estimation method. This value will be found by minimizing the sum of the squared errors of our regression. We can express the error term as follows :\n\\[\\epsilon_i=y_i-\\beta_0+\\beta_1x_i\\]\nThen we perform a summation for all the observations of our squared error term going from \\(1\\) to \\(n\\), i.e. the sum of the squared errors \\(S(\\beta_0,\\beta_1)\\).\n\\[S(\\beta_0,\\beta_1)=\\sum_{i=1}^n\\epsilon_i^2=\\sum_{i=1}^n(y_i-\\beta_0+\\beta_1 x_i)^2\\]\nThen we minimize \\(S(\\beta_0,\\beta_1)\\) with respect to \\(\\beta_0\\) and \\(\\beta_1\\). The partial derivatives with respect to \\(\\beta_0\\) is solved as follows.\n\\[\\frac{\\partial S(\\beta_0,\\beta_1)}{\\partial \\beta_0}=-2\\sum_{i=1}^n(y_i-\\beta_0-\\beta_1 x_i)\\]\nThe partial derivatives with respect to \\(\\beta_1\\) is solved as follows.\n\\[\\frac{\\partial S(\\beta_0,\\beta_1)}{\\partial \\beta_1}=-2\\sum_{i=1}^n(y_i-\\beta_0-\\beta_1 x_i)x_i\\]\nThe first order condition makes it possible to equalize our two partial derivative at 0 and we can thus find the solution.\n\\[\\frac{\\partial S(\\beta_0,\\beta_1)}{\\partial \\beta_0}=0\\]\n\\[\\frac{\\partial S(\\beta_0,\\beta_1)}{\\partial \\beta_1}=0\\]\nThe solution for the \\(\\beta_0\\) estimator is as follows:\n\\[-2\\sum_{i=1}^n(y_i-\\beta_0-\\beta_1 x_i)=0\\]\nWe can set the following properties :\n\n\\(\\sum_{i=1}^ny_i=n \\overline{y}\\)\n\\(\\sum_{i=1}^n \\beta_0=n \\beta_0\\)\n\\(\\sum_{i=1}^n \\beta_1 x_i=n \\beta_1 \\overline{x}\\)\n\n\\[n\\overline{y}-n \\beta_0-n \\beta_1 \\overline{x}=0\\]\n\\[\\beta_0=\\frac{n\\overline{y}-n \\beta_1 \\overline{x}}{n}\\]\nSolution for the estimator of \\(\\beta_0\\) :\n\\[\\hat{\\beta_0}=\\overline{y}-\\beta_1 \\overline{x}\\]\nThe solution for the \\(\\beta_1\\) estimator is as follows:\n\\[-2\\sum_{i=1}^n(y_i-\\beta_0-\\beta_1 x_i)x_i=0\\]\n\\[-2\\sum_{i=1}^n(y_i-(\\overline{y}-\\beta_1 \\overline{x})-\\beta_1 x_i)x_i=0\\]\n\\[\\sum_{i=1}^nx_iy_i-n \\overline{y} \\overline{x}+n \\beta_1 \\overline{x}^2-\\beta_1\\sum_{i=1}^nx_i^2=0\\]\n\\[\\sum_{i=1}^nx_iy_i-n \\overline{y} \\overline{x}=\\beta_1\\sum_{i=1}^nx_i^2-n \\beta_1 \\overline{x}^2\\]\n\\[\\sum_{i=1}^nx_iy_i-n \\overline{y} \\overline{x}=\\beta_1\\left(\\sum_{i=1}^nx_i^2-n \\overline{x}^2\\right)\\]\n\\[\\beta_1=\\frac{\\sum_{i=1}^nx_iy_i-n \\overline{y} \\overline{x}}{\\sum_{i=1}^nx_i^2-n \\overline{x}^2}\\]\nWe can set the following properties :\n\n\\(\\sum_{i=1}^nx_iy_i-n \\overline{y} \\overline{x}=(x_i-\\overline{x})(y_i-\\overline{y})\\)\n\\(\\sum_{i=1}^nx_i^2-n \\overline{x}^2=(x_i-\\overline{x})^2\\)\n\nSolution for the estimator of \\(\\beta_1\\) :\n\\[\\hat{\\beta_1}=\\frac{(x_i-\\overline{x})(y_i-\\overline{y})}{(x_i-\\overline{x})^2}\\]\n\n\n\nCitationBibTeX citation:@online{boucher,\n  author = {Boucher, Simon-Pierre},\n  title = {Least Squares Estimation},\n  url = {https://www.spboucher.net/posts/index5.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoucher, Simon-Pierre. n.d. “Least Squares Estimation.” https://www.spboucher.net/posts/index5.html."
  },
  {
    "objectID": "posts/index6.html",
    "href": "posts/index6.html",
    "title": "Goodness of fit",
    "section": "",
    "text": "In this chapter, we will present a measure allowing us to quantify the quality of our regression. We will start with the following three definitions:\nTotal Sum of Squares\n\\[SST=\\sum_{i=1}^n(y_i-\\overline{y})^2\\]\nRegression Sum of Squares\n\\[SSR=\\sum_{i=1}^n(\\hat{y}_i-\\overline{y})^2\\]\nError Sum of Squares\n\\[SSE=\\sum_{i=1}^n(\\hat{y}_i-y_i)^2\\]\nThe last three definitions can be related in the following equation\n\\[\\sum_{i=1}^n(y_i-\\overline{y})^2=\\sum_{i=1}^n(\\hat{y}_i-\\overline{y})^2+\\sum_{i=1}^n(\\hat{y}_i-y_i)^2\\]Or in an equivalent way\n\\[SST=SSR+SSE\\]\nOne of the most used measures is the coefficient of determination, which can be found as follows\n\\[R^2=\\frac{SSR}{SST}=1-\\frac{SSE}{SST}\\]Moreover, by construction, \\(R^2\\) will take a value between 0 and 1.:\\[R^2 \\in (0,1)\\]"
  },
  {
    "objectID": "posts/index6.html#section",
    "href": "posts/index6.html#section",
    "title": "Goodness of fit",
    "section": "",
    "text": "In this chapter, we will present a measure allowing us to quantify the quality of our regression. We will start with the following three definitions:\nTotal Sum of Squares\n\\[SST=\\sum_{i=1}^n(y_i-\\overline{y})^2\\]\nRegression Sum of Squares\n\\[SSR=\\sum_{i=1}^n(\\hat{y}_i-\\overline{y})^2\\]\nError Sum of Squares\n\\[SSE=\\sum_{i=1}^n(\\hat{y}_i-y_i)^2\\]\nThe last three definitions can be related in the following equation\n\\[\\sum_{i=1}^n(y_i-\\overline{y})^2=\\sum_{i=1}^n(\\hat{y}_i-\\overline{y})^2+\\sum_{i=1}^n(\\hat{y}_i-y_i)^2\\]Or in an equivalent way\n\\[SST=SSR+SSE\\]\nOne of the most used measures is the coefficient of determination, which can be found as follows\n\\[R^2=\\frac{SSR}{SST}=1-\\frac{SSE}{SST}\\]Moreover, by construction, \\(R^2\\) will take a value between 0 and 1.:\\[R^2 \\in (0,1)\\]"
  },
  {
    "objectID": "posts/index7.html",
    "href": "posts/index7.html",
    "title": "Assumption of the Classical Linear Regression Model",
    "section": "",
    "text": "The following assumptions must be present in order to obtain an unbiased estimator.Unbiasedness of OLS\n\\[\\hat{\\beta}=(X'X)^{-1}X'Y\\]\\[\\hat{\\beta}=(X'X)^{-1}X'(X \\beta+\\epsilon)\\]\\[\\hat{\\beta}=(X'X)^{-1}X'X \\beta +(X'X)^{-1}X' \\epsilon\\]\\[\\hat{\\beta}=\\beta+(X'X)^{-1}E(X'\\epsilon)\\]\nSince\n\\[E[\\epsilon \\mid X]=0\\]\nthe OLS estimator is unbiased\n\\[E[\\hat{\\beta}]=\\beta\\]Variance of OLS Estimators\n\\[Var(\\hat{\\beta})=Var[\\beta+(X'X)^{-1}X' \\epsilon]\\]\\[Var(\\hat{\\beta})=Var[(X'X)^{-1}X' \\epsilon]\\]\\[Var(\\hat{\\beta})=E[(X'X)^{-1}X'\\epsilon \\epsilon' X(X'X)^{-1}]\\]\\[Var(\\hat{\\beta})=(X'X)^{-1}X'E(\\epsilon \\epsilon') X(X'X)^{-1}\\]\\[Var(\\hat{\\beta})=(X'X)^{-1}X'\\sigma^2 X(X'X)^{-1}\\]Since \\[(X'X)^{-1}(X'X)=1\\]\\[Var(\\hat{\\beta})=\\sigma^2(X'X)^{-1}\\]\nAssumption of the Classical Linear Regression Model:\n1. Linearity: The model specifies a linear relationship between y and x\n\\[y=X \\beta+\\epsilon\\]2. Full rank: There is no exact linear relationship among any of the independent variables in the model.\n3. Exogeneity of the independent variables : This means that the independent variables will not carry useful information for prediction of \\(\\epsilon\\)\n\\[E[\\epsilon \\mid X]=0\\]4. Homoscedasticity and nonautocorrelation : \\(\\epsilon\\) has the same finite variance\n\\[E[\\epsilon'\\epsilon \\mid X]=\\sigma^2 I\\]\n5. Data generation: $X$ may be fixed or random.\n6. Normal distribution : The disturbances are normally distributed\n\\[\\epsilon \\mid X \\sim N(0,\\sigma^2 I)\\]\n\n\n\nCitationBibTeX citation:@online{boucher,\n  author = {Boucher, Simon-Pierre},\n  title = {Assumption of the {Classical} {Linear} {Regression} {Model}},\n  url = {https://www.spboucher.net/posts/index7.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoucher, Simon-Pierre. n.d. “Assumption of the Classical Linear\nRegression Model.” https://www.spboucher.net/posts/index7.html."
  },
  {
    "objectID": "posts/index8.html",
    "href": "posts/index8.html",
    "title": "Duproprio Webscrapping - Part 1",
    "section": "",
    "text": "This code is designed for web scraping and data collection from a real estate website (duproprio.com) and utilizes R packages such as rvest, dplyr, and glue.\n\n\nLoad the necessary libraries\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(glue)\nurl &lt;- \"\"\n\n\nCreate a loop to generate URLs for different pages of the website\nfor (i in 1:6562) {\n  url[i] &lt;- paste0(\"https://duproprio.com/en/search/list?search=true&min_price=50000&subtype%5B0%5D=1&subtype%5B1%5D=6&subtype%5B2%5D=2&subtype%5B3%5D=7&subtype%5B4%5D=5&subtype%5B5%5D=4&rooms=1&bathrooms=1&lot_dimension_sq_feet=500~&living_space_sq_feet=100~&is_sold=1&parent=1&pageNumber=\", i, \"&sort=-published_at\")\n}\n\n\nInitialize empty variables to store links and dates\nclinks &lt;- \"\"\ndates &lt;- \"\"\n\n\nLoop through the generated URLs\nfor (i in 1:6562) {\n  # Read the HTML content of the webpage\n  page &lt;- read_html(url[i])\n  \n  # Extract property links using CSS selectors\n  links &lt;- page %&gt;%\n    html_nodes(\".search-results-listings-list__item-image-link\") %&gt;%\n    html_attr(\"href\")\n  \n  # Convert the links into a data frame\n  links &lt;- as.data.frame(links)\n  \n  # Extract sold dates using CSS selectors\n  date &lt;- page %&gt;%\n    html_nodes(\".search-results-listings-list div.search-results-listings-list__item-description__item.search-results-listings-list__item-description__sold-in strong\") %&gt;%\n    html_text()\n  \n  # Convert the dates into a data frame\n  date &lt;- as.data.frame(date)\n  \n  # Append the extracted links and dates to the respective variables\n  clinks &lt;- rbind(clinks, links)\n  dates &lt;- rbind(dates, date)\n  \n  # Print a message indicating the progress\n  out &lt;- paste0(i, \"th property index page is scrapped\") \n  print(out) \n}\n\n\nCreate a data frame from the collected links and dates\ndub &lt;- as.data.frame(cbind(clinks, dates))\n\n\nIn summary, this R code is designed to scrape data from 10 pages of a real estate website. It collects property links and their associated sold dates, storing this information in data frames. The code uses various CSS selectors to locate and extract the desired data from the web pages. Finally, it combines the links and dates into a single data frame named “dub.”\n\n\n\n\nCitationBibTeX citation:@online{boucher,\n  author = {Boucher, Simon-Pierre},\n  title = {Duproprio {Webscrapping} - {Part} 1},\n  url = {https://www.spboucher.net/posts/index8.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoucher, Simon-Pierre. n.d. “Duproprio Webscrapping - Part\n1.” https://www.spboucher.net/posts/index8.html."
  },
  {
    "objectID": "posts/index9.html",
    "href": "posts/index9.html",
    "title": "Duproprio Webscrapping - Part 2",
    "section": "",
    "text": "This code is intended for web scraping from a specific real estate website (duproprio.com) and organizing the scraped data into a structured format.\n\n\nLoad a previously saved RData file named ‘dup.RData’\nload(\"dub.RData\")\n\n\nCreate an empty data frame named ‘results’ with predefined column names\nresults &lt;- data.frame(\n  url = character(),\n  price = character(),\n  type = character(),\n  location = character(),\n  bedrooms = character(),\n  bathrooms = character(),\n  livingspace = character(),\n  lotdim = character(),\n  f1 = character(),\n  f2 = character(),\n  f3 = character(),\n  f4 = character(),\n  f5 = character(),\n  f6 = character(),\n  f7 = character(),\n  f8 = character(),\n  f9 = character(),\n  f10 = character(),\n  Fd1 = character(),\n  Fd2 = character(),\n  Fd3 = character(),\n  Fd4 = character(),\n  Fd5 = character(),\n  Fd6 = character(),\n  Fd7 = character(),\n  Fd8 = character(),\n  Fd9 = character(),\n  Fd10 = character(),\n  Fd11 = character(),\n  Fd12 = character(),\n  Fd13 = character(),\n  Fd14 = character(),\n  Fd15 = character(),\n  Fd16 = character(),\n  Fd17 = character(),\n  Fd18 = character(),\n  Fd19 = character(),\n  Fd20 = character(),\n  roomdim = character(),\n  solddate = character(),\n  stringsAsFactors = FALSE\n)\n\n\nLoop to scrape data from a range of property URLs\nfor (i in 1:400) {  # You may adjust the range as needed\n  # Read the content of the webpage using the URL from the 'duproprio' dataset\n  page_content &lt;- read_html(dup$links[i])\n  \n  # Extract various property details using CSS selectors and store them in the 'results' data frame\n  results[i, 1] &lt;- dup$links[i]\n  results[i, 2] &lt;- page_content %&gt;% html_node(\".listing-sidebar &gt; .listing-information &gt; .listing-profile &gt; .listing-address &gt; .listing-price &gt; .listing-price__amount\") %&gt;% html_text()\n  results[i, 3] &lt;- page_content %&gt;% html_node(\".listing-sidebar &gt; .listing-information &gt; .listing-profile &gt; .listing-address &gt; .listing-location__title &gt; a\") %&gt;% html_text()\n  results[i,4] &lt;- page_content %&gt;% html_node(\".listing-sidebar &gt; .listing-information &gt; .listing-profile &gt; .listing-address div.listing-location__address \") %&gt;% html_text()\n  results[i,5] &lt;- page_content %&gt;% html_node(\":nth-child(1) &gt; .listing-main-characteristics__label &gt; .listing-main-characteristics__number\") %&gt;% html_text()\n  results[i,6] &lt;- page_content %&gt;% html_node(\".listing-main-characteristics__item--bathrooms &gt; .listing-main-characteristics__label &gt; .listing-main-characteristics__number\") %&gt;% html_text()\n  results[i,7] &lt;- page_content %&gt;% html_node(\".listing-main-characteristics__item--living-space-area &gt; .listing-main-characteristics__item-dimensions &gt; .listing-main-characteristics__number\") %&gt;% html_text()\n  results[i,8] &lt;- page_content %&gt;% html_node(\".listing-main-characteristics__item--lot-dimensions &gt; .listing-main-characteristics__item-dimensions &gt; .listing-main-characteristics__number\") %&gt;% html_text()\n  results[i,9] &lt;- page_content %&gt;% html_node(\".listing-list-characteristics__viewport &gt; :nth-child(2)\") %&gt;% html_text()\n  results[i,10] &lt;- page_content %&gt;% html_node(\".listing-list-characteristics__viewport &gt; :nth-child(3)\") %&gt;% html_text()\n  results[i,11] &lt;- page_content %&gt;% html_node(\".listing-list-characteristics__viewport &gt; :nth-child(4)\") %&gt;% html_text()\n  results[i,12] &lt;- page_content %&gt;% html_node(\".listing-list-characteristics__viewport &gt; :nth-child(5)\") %&gt;% html_text()\n  results[i,13] &lt;- page_content %&gt;% html_node(\".listing-list-characteristics__viewport &gt; :nth-child(6)\") %&gt;% html_text()\n  results[i,14] &lt;- page_content %&gt;% html_node(\".listing-list-characteristics__viewport &gt; :nth-child(7)\") %&gt;% html_text()\n  results[i,15] &lt;- page_content %&gt;% html_node(\".listing-list-characteristics__viewport &gt; :nth-child(8)\") %&gt;% html_text()\n  results[i,16] &lt;- page_content %&gt;% html_node(\".listing-list-characteristics__viewport &gt; :nth-child(9)\") %&gt;% html_text()\n  results[i,17] &lt;- page_content %&gt;% html_node(\".listing-list-characteristics__viewport &gt; :nth-child(10)\") %&gt;% html_text()\n  results[i,18] &lt;- page_content %&gt;% html_node(\".listing-list-characteristics__viewport &gt; :nth-child(11)\") %&gt;% html_text()\n  results[i,19] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(1)\") %&gt;% html_text()\n  results[i,20] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(2)\") %&gt;% html_text()\n  results[i,21] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(3)\") %&gt;% html_text()\n  results[i,22] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(4)\") %&gt;% html_text()\n  results[i,23] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(5)\") %&gt;% html_text()\n  results[i,24] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(6)\") %&gt;% html_text()\n  results[i,25] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(7)\") %&gt;% html_text()\n  results[i,26] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(8)\") %&gt;% html_text()\n  results[i,27] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(9)\") %&gt;% html_text()\n  results[i,28] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(10)\") %&gt;% html_text()\n  results[i,29] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(11)\") %&gt;% html_text()\n  results[i,30] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(12)\") %&gt;% html_text()\n  results[i,31] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(13)\") %&gt;% html_text()\n  results[i,32] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(14)\") %&gt;% html_text()\n  results[i,33] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(15)\") %&gt;% html_text()\n  results[i,34] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(16)\") %&gt;% html_text()\n  results[i,35] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(17)\") %&gt;% html_text()\n  results[i,36] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(18)\") %&gt;% html_text()\n  results[i,37] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(19)\") %&gt;% html_text()\n  results[i,38] &lt;- page_content %&gt;% html_node(\".listing-complete-list-characteristics__content &gt; :nth-child(20)\") %&gt;% html_text()\n  results[i,39] &lt;- page_content %&gt;% html_node(\".listing-rooms-details\") %&gt;% html_text()\n  results[i,40] &lt;- dup$date[i]\n  \n  # Print a progress message\n  out &lt;- paste0(i, \"th property is scrapped, \", 68941 - i, \" property remaining\")\n  print(out) \n}\n\n\nStore the scraped data in the ‘bd’ data frame\nbd &lt;- results\nsave(bd, file = \"bd.RData\")\n\n\nIn summary, this R code loads a dataset of property URLs, scrapes detailed information from each property’s webpage, and stores the scraped data in a structured data frame named ‘results.’ The code then saves this data frame as an RData file named ‘bd.RData’ for further analysis or use. Please note that you may need to adjust the URL range and CSS selectors based on the specific website structure and your data requirements.\n\n\n\n\nCitationBibTeX citation:@online{boucher,\n  author = {Boucher, Simon-Pierre},\n  title = {Duproprio {Webscrapping} - {Part} 2},\n  url = {https://www.spboucher.net/posts/index9.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoucher, Simon-Pierre. n.d. “Duproprio Webscrapping - Part\n2.” https://www.spboucher.net/posts/index9.html."
  },
  {
    "objectID": "posts/index10.html",
    "href": "posts/index10.html",
    "title": "Duproprio Webscrapping - Part 3",
    "section": "",
    "text": "Data preprocessing script for a dataset related to real estate or property listings. It’s likely intended to clean and transform the data to make it more suitable for analysis.\n\nThe code then proceeds with a series of data cleaning and transformation operations on the dataset “TEMP.” The dataset “TEMP” is assumed to contain various columns, such as “price,” “type,” “location,” “bedrooms,” “bathrooms,” “livingspace,” and so on. The operations are as follows:\nSeveral gsub functions are used to remove unwanted characters and spaces from the “price” column.\nSimilar gsub functions are used for the “type,” “location,” “bedrooms,” “bathrooms,” “livingspace,” and “lotdim” columns.\nThe str_match function is used to extract specific patterns from the “location,” “livingspace,” and “lotdim” columns.\nThe “YEAR” column is extracted from the “solddate” column.\nA pattern is used to extract the “CONSTYEAR” (year of construction) from various columns like “f1,” “f2,” and so on.\nSimilar patterns are used to extract various property features like “FACING,” “FLOOR,” “HEATING,” “KITCHEN,” “SHOWERANDBATH,” “BASEMENT,” “POOL,” “GAR,” and “LOCATION.”\nSeveral binary variables (e.g., “FAC_BRICK,” “FAC_STONE”) are created based on the presence of specific features in the “FACING” column.\nSimilar binary variables are created for “FLOOR,” “HEATING,” and other features.\n\nload(\"bd.RData\")\nlibrary(sjmisc)\nlibrary(stringr)\nTEMP=bd\nTEMP$price=gsub(\"\\n\",\"\",TEMP$price)\nTEMP$price=gsub(\",\",\"\",TEMP$price)\nTEMP$price=gsub(\" \",\"\",TEMP$price)\nTEMP$price=gsub(\"SalepriceprovidedbythesellertoDuProprio.\",\"\",TEMP$price)\nTEMP$price=substr(TEMP$price,2,nchar(TEMP$price))\nTEMP$type=gsub(\"\\n        \",\"\",TEMP$type)\nTEMP$type=gsub(\"\\n    \",\"\",TEMP$type)\nTEMP$location=gsub(\"\\n                \\n            \",\"%\",TEMP$location)\nTEMP$location=gsub(\"\\n            \",\"%%\",TEMP$location)\nTEMP$location=gsub(\"\\n        \\n    \",\"%%%\",TEMP$location)\nTEMP$TEMP1 &lt;- str_match(TEMP$location, \"%\\\\s*(.*?)\\\\s*%%\")\nTEMP$CITY1&lt;-TEMP$TEMP1[,2]\nTEMP$TEMP2 &lt;- str_match(TEMP$location, \"%%\\\\s*(.*?)\\\\s*%%%\")\nTEMP$CITY2&lt;-TEMP$TEMP2[,2]\nTEMP$bedrooms=gsub(\"\\n\",\"\",TEMP$bedrooms)\nTEMP$bedrooms=gsub(\" \",\"\",TEMP$bedrooms)\nTEMP$bedrooms=as.numeric(TEMP$bedrooms)\n\nTEMP$bathrooms=gsub(\"\\n\",\"\",TEMP$bathrooms)\nTEMP$bathrooms=gsub(\" \",\"\",TEMP$bathrooms)\nTEMP$bathrooms=as.numeric(TEMP$bathrooms)\nTEMP$livingspace=gsub(\"\\n                                    \",\"%\",TEMP$livingspace)\nTEMP$livingspace=gsub(\" ft²\",\"%%\",TEMP$livingspace)\nTEMP$livingspace=gsub(\" ft\",\"%%%\",TEMP$livingspace)\n\n\nTEMP$LA &lt;- str_match(TEMP$livingspace, \"%\\\\s*(.*?)\\\\s*%%\")\nTEMP$LA=gsub(\" \",\"\",TEMP$LA)\nTEMP$LA=gsub(\",\",\"\",TEMP$LA)\nTEMP$LA=TEMP$LA[,2]\n\nTEMP$LA_TEMP1 &lt;- str_match(TEMP$livingspace, \"%\\\\s*(.*?)\\\\s*x\")\nTEMP$LA_TEMP2 &lt;- str_match(TEMP$livingspace, \"x\\\\s*(.*?)\\\\s*%%%\")\nTEMP$LA_TEMP &lt;- as.numeric(TEMP$LA_TEMP1[,2])*as.numeric(TEMP$LA_TEMP2[,2])\nTEMP$LA&lt;- ifelse(is.na(TEMP$LA_TEMP),TEMP$LA ,TEMP$LA_TEMP) \nTEMP$LA=as.numeric(TEMP$LA)\n\nTEMP$lotdim=gsub(\"\\n                                \",\"%\",TEMP$lotdim)\nTEMP$lotdim=gsub(\" ft²\",\"%%\",TEMP$lotdim)\nTEMP$lotdim=gsub(\" ft\",\"%%%\",TEMP$lotdim)\n\nTEMP$LD &lt;- str_match(TEMP$lotdim, \"%\\\\s*(.*?)\\\\s*%%\")\nTEMP$LD=gsub(\" \",\"\",TEMP$LD)\nTEMP$LD=gsub(\",\",\"\",TEMP$LD)\nTEMP$LD=TEMP$LD[,2]\nTEMP$LD_TEMP1 &lt;- str_match(TEMP$lotdim, \"%\\\\s*(.*?)\\\\s*x\")\nTEMP$LD_TEMP2 &lt;- str_match(TEMP$lotdim, \"x\\\\s*(.*?)\\\\s*%%%\")\nTEMP$LD_TEMP &lt;- as.numeric(TEMP$LD_TEMP1[,2])*as.numeric(TEMP$LD_TEMP2[,2])\nTEMP$LD&lt;- ifelse(is.na(TEMP$LD_TEMP),TEMP$LD ,TEMP$LD_TEMP) \nTEMP$LD=as.numeric(TEMP$LD)\nTEMP$YEAR=substr(TEMP$solddate,nchar(TEMP$solddate)-3,nchar(TEMP$solddate))\nTEMP$YEAR_temp=as.numeric(TEMP$YEAR)\npattern&lt;-\"Year of construction\\n\"\nTEMP$CONSTYEAR&lt;- ifelse(grepl(pattern,TEMP$f1)==1, TEMP$f1, \"\") \nTEMP$CONSTYEAR&lt;- ifelse(grepl(pattern,TEMP$f2)==1, TEMP$f2, TEMP$CONSTYEAR) \nTEMP$CONSTYEAR&lt;- ifelse(grepl(pattern,TEMP$f3)==1, TEMP$f3, TEMP$CONSTYEAR) \nTEMP$CONSTYEAR&lt;- ifelse(grepl(pattern,TEMP$f4)==1, TEMP$f4, TEMP$CONSTYEAR) \nTEMP$CONSTYEAR&lt;- ifelse(grepl(pattern,TEMP$f5)==1, TEMP$f5, TEMP$CONSTYEAR) \nTEMP$CONSTYEAR&lt;- ifelse(grepl(pattern,TEMP$f6)==1, TEMP$f6, TEMP$CONSTYEAR) \nTEMP$CONSTYEAR&lt;- ifelse(grepl(pattern,TEMP$f7)==1, TEMP$f7, TEMP$CONSTYEAR) \nTEMP$CONSTYEAR&lt;- ifelse(grepl(pattern,TEMP$f8)==1, TEMP$f8, TEMP$CONSTYEAR) \nTEMP$CONSTYEAR&lt;- ifelse(grepl(pattern,TEMP$f9)==1, TEMP$f9, TEMP$CONSTYEAR) \nTEMP$CONSTYEAR&lt;- ifelse(grepl(pattern,TEMP$f10)==1, TEMP$f10, TEMP$CONSTYEAR) \nTEMP$CONSTYEAR=gsub(\"\\n                    Year of construction\\n                    \\n                    \",\"\",TEMP$CONSTYEAR)\nTEMP$CONSTYEAR=gsub(\"\\n                \",\"\",TEMP$CONSTYEAR)\nTEMP$CONSTYEAR=as.numeric(TEMP$CONSTYEAR)\nTEMP$AGE=ifelse(grepl(\"New\",TEMP$type)==1,0 ,TEMP$YEAR_temp-TEMP$CONSTYEAR) \n\npattern&lt;-\"External facing:\\n\"\nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd1)==1, TEMP$Fd1, \"\") \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd2)==1, TEMP$Fd2, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd3)==1, TEMP$Fd3, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd4)==1, TEMP$Fd4, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd5)==1, TEMP$Fd5, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd6)==1, TEMP$Fd6, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd7)==1, TEMP$Fd7, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd8)==1, TEMP$Fd8, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd9)==1, TEMP$Fd9, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd10)==1, TEMP$Fd10, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd11)==1, TEMP$Fd11, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd12)==1, TEMP$Fd12, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd13)==1, TEMP$Fd13, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd14)==1, TEMP$Fd14, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd15)==1, TEMP$Fd15, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd16)==1, TEMP$Fd16, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd17)==1, TEMP$Fd17, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd18)==1, TEMP$Fd18, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd19)==1, TEMP$Fd19, TEMP$FACING) \nTEMP$FACING&lt;- ifelse(grepl(pattern,TEMP$Fd20)==1, TEMP$Fd20, TEMP$FACING) \n\npattern&lt;-\"Floor coverings:\\n\"\nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd1)==1, TEMP$Fd1, \"\") \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd2)==1, TEMP$Fd2, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd3)==1, TEMP$Fd3, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd4)==1, TEMP$Fd4, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd5)==1, TEMP$Fd5, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd6)==1, TEMP$Fd6, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd7)==1, TEMP$Fd7, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd8)==1, TEMP$Fd8, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd9)==1, TEMP$Fd9, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd10)==1, TEMP$Fd10, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd11)==1, TEMP$Fd11, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd12)==1, TEMP$Fd12, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd13)==1, TEMP$Fd13, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd14)==1, TEMP$Fd14, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd15)==1, TEMP$Fd15, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd16)==1, TEMP$Fd16, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd17)==1, TEMP$Fd17, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd18)==1, TEMP$Fd18, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd19)==1, TEMP$Fd19, TEMP$FLOOR) \nTEMP$FLOOR&lt;- ifelse(grepl(pattern,TEMP$Fd20)==1, TEMP$Fd20, TEMP$FLOOR) \n\npattern&lt;-\"Heating source:\\n\"\nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd1)==1, TEMP$Fd1, \"\") \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd2)==1, TEMP$Fd2, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd3)==1, TEMP$Fd3, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd4)==1, TEMP$Fd4, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd5)==1, TEMP$Fd5, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd6)==1, TEMP$Fd6, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd7)==1, TEMP$Fd7, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd8)==1, TEMP$Fd8, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd9)==1, TEMP$Fd9, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd10)==1, TEMP$Fd10, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd11)==1, TEMP$Fd11, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd12)==1, TEMP$Fd12, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd13)==1, TEMP$Fd13, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd14)==1, TEMP$Fd14, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd15)==1, TEMP$Fd15, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd16)==1, TEMP$Fd16, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd17)==1, TEMP$Fd17, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd18)==1, TEMP$Fd18, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd19)==1, TEMP$Fd19, TEMP$HEATING) \nTEMP$HEATING&lt;- ifelse(grepl(pattern,TEMP$Fd20)==1, TEMP$Fd20, TEMP$HEATING) \n\npattern&lt;-\"Kitchen:\\n\"\nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd1)==1, TEMP$Fd1, \"\") \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd2)==1, TEMP$Fd2, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd3)==1, TEMP$Fd3, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd4)==1, TEMP$Fd4, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd5)==1, TEMP$Fd5, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd6)==1, TEMP$Fd6, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd7)==1, TEMP$Fd7, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd8)==1, TEMP$Fd8, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd9)==1, TEMP$Fd9, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd10)==1, TEMP$Fd10, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd11)==1, TEMP$Fd11, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd12)==1, TEMP$Fd12, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd13)==1, TEMP$Fd13, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd14)==1, TEMP$Fd14, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd15)==1, TEMP$Fd15, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd16)==1, TEMP$Fd16, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd17)==1, TEMP$Fd17, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd18)==1, TEMP$Fd18, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd19)==1, TEMP$Fd19, TEMP$KITCHEN) \nTEMP$KITCHEN&lt;- ifelse(grepl(pattern,TEMP$Fd20)==1, TEMP$Fd20, TEMP$KITCHEN) \n\npattern&lt;-\"Bathroom:\\n\"\nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd1)==1, TEMP$Fd1, \"\") \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd2)==1, TEMP$Fd2, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd3)==1, TEMP$Fd3, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd4)==1, TEMP$Fd4, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd5)==1, TEMP$Fd5, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd6)==1, TEMP$Fd6, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd7)==1, TEMP$Fd7, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd8)==1, TEMP$Fd8, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd9)==1, TEMP$Fd9, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd10)==1, TEMP$Fd10, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd11)==1, TEMP$Fd11, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd12)==1, TEMP$Fd12, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd13)==1, TEMP$Fd13, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd14)==1, TEMP$Fd14, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd15)==1, TEMP$Fd15, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd16)==1, TEMP$Fd16, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd17)==1, TEMP$Fd17, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd18)==1, TEMP$Fd18, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd19)==1, TEMP$Fd19, TEMP$SHOWERANDBATH) \nTEMP$SHOWERANDBATH&lt;- ifelse(grepl(pattern,TEMP$Fd20)==1, TEMP$Fd20, TEMP$SHOWERANDBATH) \n\n\npattern&lt;-\"Basement:\\n\"\nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd1)==1, TEMP$Fd1, \"\") \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd2)==1, TEMP$Fd2, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd3)==1, TEMP$Fd3, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd4)==1, TEMP$Fd4, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd5)==1, TEMP$Fd5, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd6)==1, TEMP$Fd6, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd7)==1, TEMP$Fd7, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd8)==1, TEMP$Fd8, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd9)==1, TEMP$Fd9, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd10)==1, TEMP$Fd10, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd11)==1, TEMP$Fd11, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd12)==1, TEMP$Fd12, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd13)==1, TEMP$Fd13, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd14)==1, TEMP$Fd14, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd15)==1, TEMP$Fd15, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd16)==1, TEMP$Fd16, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd17)==1, TEMP$Fd17, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd18)==1, TEMP$Fd18, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd19)==1, TEMP$Fd19, TEMP$BASEMENT) \nTEMP$BASEMENT&lt;- ifelse(grepl(pattern,TEMP$Fd20)==1, TEMP$Fd20, TEMP$BASEMENT) \n\n\npattern&lt;-\"Pool:\\n\"\nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd1)==1, TEMP$Fd1, \"\") \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd2)==1, TEMP$Fd2, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd3)==1, TEMP$Fd3, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd4)==1, TEMP$Fd4, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd5)==1, TEMP$Fd5, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd6)==1, TEMP$Fd6, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd7)==1, TEMP$Fd7, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd8)==1, TEMP$Fd8, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd9)==1, TEMP$Fd9, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd10)==1, TEMP$Fd10, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd11)==1, TEMP$Fd11, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd12)==1, TEMP$Fd12, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd13)==1, TEMP$Fd13, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd14)==1, TEMP$Fd14, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd15)==1, TEMP$Fd15, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd16)==1, TEMP$Fd16, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd17)==1, TEMP$Fd17, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd18)==1, TEMP$Fd18, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd19)==1, TEMP$Fd19, TEMP$POOL) \nTEMP$POOL&lt;- ifelse(grepl(pattern,TEMP$Fd20)==1, TEMP$Fd20, TEMP$POOL) \n\n\npattern&lt;-\"Garage:\\n\"\nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd1)==1, TEMP$Fd1, \"\") \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd2)==1, TEMP$Fd2, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd3)==1, TEMP$Fd3, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd4)==1, TEMP$Fd4, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd5)==1, TEMP$Fd5, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd6)==1, TEMP$Fd6, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd7)==1, TEMP$Fd7, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd8)==1, TEMP$Fd8, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd9)==1, TEMP$Fd9, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd10)==1, TEMP$Fd10, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd11)==1, TEMP$Fd11, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd12)==1, TEMP$Fd12, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd13)==1, TEMP$Fd13, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd14)==1, TEMP$Fd14, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd15)==1, TEMP$Fd15, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd16)==1, TEMP$Fd16, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd17)==1, TEMP$Fd17, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd18)==1, TEMP$Fd18, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd19)==1, TEMP$Fd19, TEMP$GAR) \nTEMP$GAR&lt;- ifelse(grepl(pattern,TEMP$Fd20)==1, TEMP$Fd20, TEMP$GAR) \n\npattern&lt;-\"Location:\\n\"\nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd1)==1, TEMP$Fd1, \"\") \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd2)==1, TEMP$Fd2, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd3)==1, TEMP$Fd3, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd4)==1, TEMP$Fd4, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd5)==1, TEMP$Fd5, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd6)==1, TEMP$Fd6, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd7)==1, TEMP$Fd7, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd8)==1, TEMP$Fd8, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd9)==1, TEMP$Fd9, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd10)==1, TEMP$Fd10, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd11)==1, TEMP$Fd11, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd12)==1, TEMP$Fd12, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd13)==1, TEMP$Fd13, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd14)==1, TEMP$Fd14, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd15)==1, TEMP$Fd15, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd16)==1, TEMP$Fd16, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd17)==1, TEMP$Fd17, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd18)==1, TEMP$Fd18, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd19)==1, TEMP$Fd19, TEMP$LOCATION) \nTEMP$LOCATION&lt;- ifelse(grepl(pattern,TEMP$Fd20)==1, TEMP$Fd20, TEMP$LOCATION) \n\nTEMP$FAC_BRICK&lt;- ifelse(grepl(\"Brick\\n\",TEMP$FACING)==1, 1, 0) \nTEMP$FAC_STONE&lt;- ifelse(grepl(\"Stone\\n\",TEMP$FACING)==1, 1, 0) \nTEMP$FAC_VINYL&lt;- ifelse(grepl(\"Vinyl Siding\\n\",TEMP$FACING)==1, 1, 0)\nTEMP$FAC_WOOD&lt;- ifelse(grepl(\"Wood\\n\",TEMP$FACING)==1, 1, 0)\nTEMP$FAC_ALLUMINUIUM&lt;- ifelse(grepl(\"Aluminium Siding\\n\",TEMP$FACING)==1, 1, 0)\n\n\n\nTEMP$FLOOR_CERAMIC&lt;- ifelse(grepl(\"Ceramic\\n\",TEMP$FLOOR)==1, 1, 0) \nTEMP$FLOOR_HARDWOOD&lt;- ifelse(grepl(\"Hardwood\\n\",TEMP$FLOOR)==1, 1, 0) \nTEMP$FLOOR_LAMINATE&lt;- ifelse(grepl(\"Laminate\\n\",TEMP$FLOOR)==1, 1, 0)\n\nTEMP$HEATING_FORCEDAIR&lt;- ifelse(grepl(\"Forced air\\n\",TEMP$HEATING)==1, 1, 0) \nTEMP$HEATING_NATURALGAS&lt;- ifelse(grepl(\"Natural gas\\n\",TEMP$HEATING)==1, 1, 0) \nTEMP$HEATING_ELECTRICITY&lt;- ifelse(grepl(\"Electric\\n\",TEMP$HEATING)==1, 1, 0)\n\nTEMP$BASEMENT_FINISH&lt;- ifelse(grepl(\"Totally finished\\n\",TEMP$BASEMENT)==1, 1, 0) \n\nTEMP$POOL_ABOVE&lt;- ifelse(grepl(\"Above ground\\n\",TEMP$POOL)==1, 1, 0) \nTEMP$POOL_IN&lt;- ifelse(grepl(\"Inground\\n\",TEMP$POOL)==1, 1, 0) \n\nTEMP$GAR_SINGLE&lt;- ifelse(grepl(\"Single\\n\",TEMP$GAR)==1, 1, 0) \nTEMP$GAR_DOUBLE&lt;- ifelse(grepl(\"Double\\n\",TEMP$GAR)==1, 1, 0) \n\n\nTEMP$LOCATION_NOBACK&lt;- ifelse(grepl(\"No backyard neighbors\\n\",TEMP$LOCATION)==1, 1, 0) \nTEMP$LOCATION_RESIDENTIAL&lt;- ifelse(grepl(\"Residential area\\n\",TEMP$LOCATION)==1, 1, 0) \nTEMP$LOCATION_PARK&lt;- ifelse(grepl(\"Near park\\n\",TEMP$LOCATION)==1, 1, 0) \nTEMP$LOCATION_PUBLICT&lt;- ifelse(grepl(\"Public transportation\\n\",TEMP$LOCATION)==1, 1, 0) \nTEMP$LOCATION_HIGHWAY&lt;- ifelse(grepl(\"Highway access\\n\",TEMP$LOCATION)==1, 1, 0) \nTEMP$KITCHEN_ISLAND&lt;- ifelse(grepl(\"Island\\n\",TEMP$KITCHEN)==1, 1, 0) \n\n\nTEMP$type=ifelse(TEMP$type==\"New 2 Storey\", \"2 Storey\",TEMP$type)\nTEMP$type=ifelse(TEMP$type==\"New Bungalow\", \"Bungalow\",TEMP$type)\nTEMP$type=ifelse(TEMP$type==\"New Semi-detached\", \"Semi-detached\",TEMP$type)\nTEMP$type=ifelse(TEMP$type==\"New Townhouse\", \"Townhouse\",TEMP$type)\nTEMP$type=ifelse(TEMP$type==\"New 1  1/2  Storey\", \"1  1/2  Storey\",TEMP$type)\n\n\n\nDB_REGRESS=as.data.frame(cbind(TEMP$price,TEMP$type,TEMP$CITY1,TEMP$bedrooms,TEMP$bathrooms,TEMP$LA,TEMP$LD,TEMP$AGE,TEMP$YEAR,TEMP$POOL_ABOVE,TEMP$POOL_IN,TEMP$FAC_BRICK,TEMP$FAC_STONE,TEMP$FLOOR_CERAMIC,TEMP$FLOOR_HARDWOOD,TEMP$GAR_DOUBLE,TEMP$GAR_SINGLE,TEMP$HEATING_ELECTRICITY,TEMP$HEATING_NATURALGAS,TEMP$BASEMENT_FINISH,TEMP$LOCATION_HIGHWAY,TEMP$LOCATION_NOBACK,TEMP$LOCATION_RESIDENTIAL,TEMP$LOCATION_PUBLICT,TEMP$KITCHEN_ISLAND))\nnames(DB_REGRESS)[1]&lt;-\"PRICE\"\nnames(DB_REGRESS)[2]&lt;-\"TYPE\"\nnames(DB_REGRESS)[3]&lt;-\"CITY\"\nnames(DB_REGRESS)[4]&lt;-\"BED\"\nnames(DB_REGRESS)[5]&lt;-\"BATH\"\nnames(DB_REGRESS)[6]&lt;-\"LA\"\nnames(DB_REGRESS)[7]&lt;-\"LD\"\nnames(DB_REGRESS)[8]&lt;-\"AGE\"\nnames(DB_REGRESS)[9]&lt;-\"YOS\"\nnames(DB_REGRESS)[10]&lt;-\"POOL_ABOVE\"\nnames(DB_REGRESS)[11]&lt;-\"POOL_IN\"\nnames(DB_REGRESS)[12]&lt;-\"BRICK\"\nnames(DB_REGRESS)[13]&lt;-\"STONE\"\nnames(DB_REGRESS)[14]&lt;-\"CERAMIC\"\nnames(DB_REGRESS)[15]&lt;-\"HARDWOOD\"\nnames(DB_REGRESS)[16]&lt;-\"GAR_DOUBLE\"\nnames(DB_REGRESS)[17]&lt;-\"GAR_SINGLE\"\nnames(DB_REGRESS)[18]&lt;-\"ELECTRICITY\"\nnames(DB_REGRESS)[19]&lt;-\"NATURALGAS\"\nnames(DB_REGRESS)[20]&lt;-\"BASEMENT_FINISH\"\nnames(DB_REGRESS)[21]&lt;-\"HIGHWAY\"\nnames(DB_REGRESS)[22]&lt;-\"NOBACK\"\nnames(DB_REGRESS)[23]&lt;-\"RESIDENTIAL\"\nnames(DB_REGRESS)[24]&lt;-\"PUBLICT\"\nnames(DB_REGRESS)[25]&lt;-\"KITCHENISLAND\"\n\nDB_REGRESS$PRICE=as.numeric(DB_REGRESS$PRICE)\nDB_REGRESS$BED=as.numeric(DB_REGRESS$BED)\nDB_REGRESS$BATH=as.numeric(DB_REGRESS$BATH)\nDB_REGRESS$LA=as.numeric(DB_REGRESS$LA)\nDB_REGRESS$LD=as.numeric(DB_REGRESS$LD)\nDB_REGRESS$AGE=as.numeric(DB_REGRESS$AGE) \nDB_REGRESS$TYPE=as.factor(DB_REGRESS$TYPE)\nDB_REGRESS$YOS=as.factor(DB_REGRESS$YOS)\nDB_REGRESS$CITY=as.factor(DB_REGRESS$CITY)\nYY=na.omit(DB_REGRESS)\nsave(YY,file = \"BD_HEDONIC.Rdata\")\nHOUSING_MODEL&lt;-lm(PRICE ~ BED + BATH + LA + LD + AGE + POOL_ABOVE + POOL_IN + GAR_DOUBLE + GAR_SINGLE + BASEMENT_FINISH + BRICK + STONE + CERAMIC + HARDWOOD + HIGHWAY + NOBACK + RESIDENTIAL + PUBLICT+ KITCHENISLAND + TYPE + YOS + CITY ,data = YY)\nsummary(HOUSING_MODEL)\nHEDONIC_COEF=as.data.frame(HOUSING_MODEL$coefficients)\n\nHEDONIC_COEF$NAME=row.names(HEDONIC_COEF)\nwrite.table(HEDONIC_COEF,file = \"HEDONIC_COEF.txt\")\n\n\nThe code performed data cleaning and feature engineering on a real estate dataset. It creates new variables and cleans existing ones to prepare the data for analysis or modeling.\n\n\n\n\nCitationBibTeX citation:@online{boucher,\n  author = {Boucher, Simon-Pierre},\n  title = {Duproprio {Webscrapping} - {Part} 3},\n  url = {https://www.spboucher.net/posts/index10.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoucher, Simon-Pierre. n.d. “Duproprio Webscrapping - Part\n3.” https://www.spboucher.net/posts/index10.html."
  }
]