[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simon-Pierre Boucher",
    "section": "",
    "text": "Greetings! I’m a doctoral candidate at the University Laval’s Department of Finance, Insurance and Real Estate, where I’m pursuing a PhD in finance. My main area of research and academic focus is on econometric methods related to time series, high-frequency data modeling, and the impact of macroeconomic announcements, as well as commodity financialization.\nTo study high-frequency financial data, I rely on the programming language R as my primary tool. At present, I’m working on my thesis, and I’m excited to be making progress on this significant research undertaking."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Simon-Pierre Boucher",
    "section": "",
    "text": "EDUCATION\n\nDoctorate in Administrative Sciences - Finance and Insurance (Ph.D.)\n\nLaval University, Quebec\n2019-present\n\n\n\nMaster of Science in Administration - Finance (M. Sc)\n\nLaval University, Quebec\n2017-2018\n\n\n\nBachelor of Business Administration - Finance (B.A.A.)\n\nLaval University, Quebec\n2013-2017\n\n\n\n\nEXPERIENCE\n\nLaval University\n\nTeaching\n\nGSF-3100 Capital market (Fall/Winter 2021, Fall 2022 and Winter 2023)\nGSF-6053 Financial Econometrics (Winter 2022)\nGSF-1500 Financial Management (Summer 2022)\n\n\n\nGraduate Teaching Assistant\n\nGSF-2101 Portfolio Management\nGSF-6025 Financial Strategies and Policies I\nGSF-6008 Corporate finance\nGSF-2102 Corporate finance\nGSF-6028 Financial Theory\n\n\n\nGraduate Research Assistant\n\n2017 - 2021\nReal estate database creation\nHedonic modeling\nLiterature review on financial problematics\nProgram econometric regression models on R, SAS, MATLAB.\n\n\n\n\n\nRESEARCH PROJECTS\n\nHas financialization changed the impact of macro announcements ?\nDo macro announcement surprises cause changes in liquidity, volatility or arbitrage opportunity in the Exchanged Traded Fund market?\n\n\n\nTECHNICAL STRENGTHS\n\nComputer Languages: SAS, R, MatLab, STATA, Python, SQL, Latex\nDatabases: COMPUSTAT, CRSP, Bloomberg, Morningstar, Thomson Reuters"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Author: Boucher, Simon-Pierre, Gagnon, Marie-Hélène and Power, Gabriel\nKeywords: commodities; futures; spillover; financialization; high-frequency; commercial; institutional; volatility; macro; announcements; surprise; events\nJEL Classification: E44, G13, G14, G15\nAbstract: We investigate, using high-frequency data, how financialization has changed the im- pact of macroeconomic announcements on commodity futures returns and volatility. We find that greater financialization dampens the impact of macroeconomic release surprises on commodity markets, as measured by price drift and volatility changes. Moreover, financial participants improve liquidity and price discovery, while reduc- ing volatility. Since traditional market participants prefer stability, our results suggest a beneficial impact of financialization. When we disaggregate the results, we find that the beneficial effects of greater financial participation are linked to money managers rather than swap dealers.\nLink\n\n\n\n\n\nAuthor: Boucher, Simon-Pierre, Gagnon, Marie-Hélène and Power, Gabriel\nKeywords: commodities; futures; spillover; high-frequency; volatility\nJEL Classification: E44, G13, G14, G15\nAbstract: In this research, we investigate the volatility dynamics in the commodity exchange- traded fund (ETF) market, a rapidly expanding sector. We utilize the Bayesian Vector Autoregression (BVAR) and Heterogeneous Autoregressive (HAR) models to gain in- sights into market behavior. Our analysis confirms the effectiveness of both models in capturing volatility dynamics. Particularly, we emphasize the influence of commod- ity prices on the volatility of commodity ETFs, underlining their interlinked nature. In the BVAR model, we observe a predominant spillover effect from the indicative net asset value (iNAV) to the ETF. This suggests that the iNAV, more than just reflecting market movements, actively influences the ETF’s volatility. The HAR model, with its time-varying betas, provides a detailed view of the changing risk exposures in this market, complementing our understanding of these dynamics. Our study contributes to existing literature by developing an intraday series for the ETF’s NAV, or iNAV. We document a bidirectional spillover between the ETF and its iNAV for ETFs backed by energy futures contracts, while a more pronounced spillover from iNAV to ETF is observed for those backed by metal futures contracts.\nLink\n\n\n\n\n\nAuthor: Boucher, Simon-Pierre, Gagnon, Marie-Hélène and Power, Gabriel\nKeywords: commodities; futures; spillover; high-frequency; volatility\nJEL Classification: E44, G13, G14, G15\nAbstract: In this study, we investigate the high-frequency impact of macroeconomic announce- ments on the volatility of key futures contracts. We use three estimators of spot volatility: intraday periodicity, kernel estimation and the intraday GARCH model. Our main objective is to determine which of these estimators best captures the mar- ket’s reaction to macroeconomic announcements. Our results show that the intra- day GARCH model stands out as the most effective in capturing market responses to macroeconomic announcements.\nLink"
  },
  {
    "objectID": "research.html#working-paper",
    "href": "research.html#working-paper",
    "title": "Research",
    "section": "",
    "text": "Author: Boucher, Simon-Pierre, Gagnon, Marie-Hélène and Power, Gabriel\nKeywords: commodities; futures; spillover; financialization; high-frequency; commercial; institutional; volatility; macro; announcements; surprise; events\nJEL Classification: E44, G13, G14, G15\nAbstract: We investigate, using high-frequency data, how financialization has changed the im- pact of macroeconomic announcements on commodity futures returns and volatility. We find that greater financialization dampens the impact of macroeconomic release surprises on commodity markets, as measured by price drift and volatility changes. Moreover, financial participants improve liquidity and price discovery, while reduc- ing volatility. Since traditional market participants prefer stability, our results suggest a beneficial impact of financialization. When we disaggregate the results, we find that the beneficial effects of greater financial participation are linked to money managers rather than swap dealers.\nLink\n\n\n\n\n\nAuthor: Boucher, Simon-Pierre, Gagnon, Marie-Hélène and Power, Gabriel\nKeywords: commodities; futures; spillover; high-frequency; volatility\nJEL Classification: E44, G13, G14, G15\nAbstract: In this research, we investigate the volatility dynamics in the commodity exchange- traded fund (ETF) market, a rapidly expanding sector. We utilize the Bayesian Vector Autoregression (BVAR) and Heterogeneous Autoregressive (HAR) models to gain in- sights into market behavior. Our analysis confirms the effectiveness of both models in capturing volatility dynamics. Particularly, we emphasize the influence of commod- ity prices on the volatility of commodity ETFs, underlining their interlinked nature. In the BVAR model, we observe a predominant spillover effect from the indicative net asset value (iNAV) to the ETF. This suggests that the iNAV, more than just reflecting market movements, actively influences the ETF’s volatility. The HAR model, with its time-varying betas, provides a detailed view of the changing risk exposures in this market, complementing our understanding of these dynamics. Our study contributes to existing literature by developing an intraday series for the ETF’s NAV, or iNAV. We document a bidirectional spillover between the ETF and its iNAV for ETFs backed by energy futures contracts, while a more pronounced spillover from iNAV to ETF is observed for those backed by metal futures contracts.\nLink\n\n\n\n\n\nAuthor: Boucher, Simon-Pierre, Gagnon, Marie-Hélène and Power, Gabriel\nKeywords: commodities; futures; spillover; high-frequency; volatility\nJEL Classification: E44, G13, G14, G15\nAbstract: In this study, we investigate the high-frequency impact of macroeconomic announce- ments on the volatility of key futures contracts. We use three estimators of spot volatility: intraday periodicity, kernel estimation and the intraday GARCH model. Our main objective is to determine which of these estimators best captures the mar- ket’s reaction to macroeconomic announcements. Our results show that the intra- day GARCH model stands out as the most effective in capturing market responses to macroeconomic announcements.\nLink"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "GSF-3100 Capital market (Fall/Winter 2021)\nThe objective of this course is to analyze capital markets, their main financial instruments and their roles in the intermediation of funds and risk; to examine the role of financial institutions and supervisory agencies, particularly in the Canadian context; analyze the term structure of interest rates and the volatility of fixed income securities, government, corporate and international bond markets; study fixed income securities with optional clauses and the main financial markets for risk intermediation classified according to the instruments traded in them, i.e., asset-backed securities, futures, options and swaps markets\n\nSlide S01\nSlide S02\nSlide S03\nSlide S04\nSlide S05\nSlide S06\nSlide S07\nSlide S08\nSlide S09A\nSlide S09B\nSlide S09C\nSlide S10A\nSlide S10B\n\nGSF-6053 Financial Econometrics (Winter 2022)\nThis course familiarizes the student with the many practical dimensions of the use of econometric methods and estimation techniques in finance. Emphasis is placed on modeling problems and financial applications. Basic econometric models and concepts related to financial markets are presented. The student is expected to have acquired a basic knowledge of statistics.\n\nSlide S02\nSlide S03\nSlide S04\nSlide S05\nSlide S06\nSlide S07\nSlide S08\nSlide S09\nSlide S10\nSlide S11\nSlide S12\n\nGSF-1500 Financial Management (Summer 2022)\nThe course aims to introduce the future administrator to the principles and techniques of modern financial management. The teaching of theoretical models is focused on the current practice of corporate financial management. It serves mainly to develop the essential tools for decision-making. The principles and tools developed in this course are critically applied to key financial decisions: the choice of investments, the evaluation of the cost of capital and the choice of permanent financing (corporate capital structure and dividend policy)."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "R script is designed to plot a budget constraint and an indifference curve",
    "section": "",
    "text": "This R script is designed to plot a budget constraint and an indifference curve for an intertemporal choice scenario within the context of financial theory. Initially, the script defines incomes for two periods (`y0` and `y1`), as well as the interest rate. It then calculates the total available amount for consumption (`W0`) by accounting for the interest rate to discount the income from year 1. This calculation ensures that the future value of money is appropriately adjusted for present value.\nSubsequently, the script generates a linear budget constraint that illustrates how consumption in year 0 and year 1 must adjust to stay within the total budget. This is depicted by plotting `x_ajuste` against `y_ajuste`, where these variables represent consumption in year 0 and year 1, respectively, adjusted across 100 points to demonstrate the trade-off between present and future consumption.\nMoreover, the script employs a logarithmic utility function to calculate and plot an indifference curve. This curve represents combinations of consumption between years 0 and 1 that provide the same level of utility. To do this, it first calculates a feasible level of utility (`utilite_realisable`) using a selected budget point (`c0_budget`, `c1_budget`) as a reference. Then, it plots a series of points (`c0_points_filtered`, `c1_points_filtered`) that achieve this fixed level of utility across different consumption mixes.\nThe final plot visually compares these two crucial concepts in financial theory: the linear budget constraint (in blue) and the curved indifference line (in red), against a backdrop of possible consumption choices in years 0 and 1. The plot effectively demonstrates the trade-offs and decision-making process involved in intertemporal consumption planning, highlighting the impact of interest rates on present and future consumption choices.\n\n# GSF-6028\n# THÉORIE FINANCIERE\n\nlibrary(ggplot2)\ny0=500\ny1=500\ntaux_interet=0.05\n\nW0 &lt;- y0 + y1 / (1 + taux_interet)\n\n\nx_ajuste &lt;- seq(0, W0, length.out = 100)  \ny_ajuste &lt;- (W0 - x_ajuste) * (1 + taux_interet)  \n\n\n\n\ndf &lt;- data.frame(x_ajuste, y_ajuste)\n\nc0_budget &lt;- W0 / 2\nc1_budget &lt;- (W0 - c0_budget) * (1 + taux_interet)\nutilite_log &lt;- function(c0, c1) {\n\n  return(log(c0) + (1/(1+taux_interet))*log(c1))\n}\n\nutilite_realisable &lt;- utilite_log(c0_budget, c1_budget)\n\ntrouver_c1_pour_utilite_fixe &lt;- function(c0, utilite_fixe) {\n  f &lt;- function(c1) utilite_log(c0, c1) - utilite_fixe\n  tryCatch({\n    return(uniroot(f, c(0.1, W0))$root)\n  }, error=function(e) {\n    return(NaN)\n  })\n}\n\nc0_points &lt;- seq(0, 1000, by = 10)\nc1_points_realisable &lt;- sapply(c0_points, function(c0) trouver_c1_pour_utilite_fixe(c0, utilite_realisable))\n\nc0_points_filtered &lt;- c0_points[!is.nan(c1_points_realisable)]\nc1_points_filtered &lt;- c1_points_realisable[!is.nan(c1_points_realisable)]\n\nplot(c0_points_filtered, c1_points_filtered, type = \"l\", col = \"red\", \n     xlab = \"Consommation année 0 (y0)\", ylab = \"Consommation année 1 (y1)\",\n     main = \"Contrainte de Richesse et Courbe d'Indifférence Ajustée\",xlim = c(0,1000),ylim = c(0,1000),lwd=3)\nlines(x_ajuste,y_ajuste,lwd=3,col=\"blue\")\ngrid()\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{boucher,\n  author = {Boucher, Simon-Pierre},\n  title = {R Script Is Designed to Plot a Budget Constraint and an\n    Indifference Curve},\n  url = {https://www.spboucher.net/posts},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoucher, Simon-Pierre. n.d. “R Script Is Designed to Plot a Budget\nConstraint and an Indifference Curve .” https://www.spboucher.net/posts."
  },
  {
    "objectID": "posts/index.html#quarto",
    "href": "posts/index.html#quarto",
    "title": "Untitled",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "posts/index.html#running-code",
    "href": "posts/index.html#running-code",
    "title": "Untitled",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nR script is designed to plot a budget constraint and an indifference curve\n\n\n\n\n\n\n\n\n\n\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nR function that computes all the main descriptive statistics\n\n\n\n\n\n\n\n\n\n\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nSimple Linear Model\n\n\n\n\n\n\n\n\n\n\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nLeast squares: two or more independent variables\n\n\n\n\n\n\n\n\n\n\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nLeast squares estimation\n\n\n\n\n\n\n\n\n\n\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/index.html#r-script-is-designed-to-plot-a-budget-constraint-and-an-indifference-curve",
    "href": "posts/index.html#r-script-is-designed-to-plot-a-budget-constraint-and-an-indifference-curve",
    "title": "R script is designed to plot a budget constraint and an indifference curve",
    "section": "",
    "text": "This R script is designed to plot a budget constraint and an indifference curve for an intertemporal choice scenario within the context of financial theory. Initially, the script defines incomes for two periods (`y0` and `y1`), as well as the interest rate. It then calculates the total available amount for consumption (`W0`) by accounting for the interest rate to discount the income from year 1. This calculation ensures that the future value of money is appropriately adjusted for present value.\nSubsequently, the script generates a linear budget constraint that illustrates how consumption in year 0 and year 1 must adjust to stay within the total budget. This is depicted by plotting `x_ajuste` against `y_ajuste`, where these variables represent consumption in year 0 and year 1, respectively, adjusted across 100 points to demonstrate the trade-off between present and future consumption.\nMoreover, the script employs a logarithmic utility function to calculate and plot an indifference curve. This curve represents combinations of consumption between years 0 and 1 that provide the same level of utility. To do this, it first calculates a feasible level of utility (`utilite_realisable`) using a selected budget point (`c0_budget`, `c1_budget`) as a reference. Then, it plots a series of points (`c0_points_filtered`, `c1_points_filtered`) that achieve this fixed level of utility across different consumption mixes.\nThe final plot visually compares these two crucial concepts in financial theory: the linear budget constraint (in blue) and the curved indifference line (in red), against a backdrop of possible consumption choices in years 0 and 1. The plot effectively demonstrates the trade-offs and decision-making process involved in intertemporal consumption planning, highlighting the impact of interest rates on present and future consumption choices.\n\n# GSF-6028\n# THÉORIE FINANCIERE\n\nlibrary(ggplot2)\ny0=500\ny1=500\ntaux_interet=0.05\n\nW0 &lt;- y0 + y1 / (1 + taux_interet)\n\n\nx_ajuste &lt;- seq(0, W0, length.out = 100)  \ny_ajuste &lt;- (W0 - x_ajuste) * (1 + taux_interet)  \n\n\n\n\ndf &lt;- data.frame(x_ajuste, y_ajuste)\n\nc0_budget &lt;- W0 / 2\nc1_budget &lt;- (W0 - c0_budget) * (1 + taux_interet)\nutilite_log &lt;- function(c0, c1) {\n\n  return(log(c0) + (1/(1+taux_interet))*log(c1))\n}\n\nutilite_realisable &lt;- utilite_log(c0_budget, c1_budget)\n\ntrouver_c1_pour_utilite_fixe &lt;- function(c0, utilite_fixe) {\n  f &lt;- function(c1) utilite_log(c0, c1) - utilite_fixe\n  tryCatch({\n    return(uniroot(f, c(0.1, W0))$root)\n  }, error=function(e) {\n    return(NaN)\n  })\n}\n\nc0_points &lt;- seq(0, 1000, by = 10)\nc1_points_realisable &lt;- sapply(c0_points, function(c0) trouver_c1_pour_utilite_fixe(c0, utilite_realisable))\n\nc0_points_filtered &lt;- c0_points[!is.nan(c1_points_realisable)]\nc1_points_filtered &lt;- c1_points_realisable[!is.nan(c1_points_realisable)]\n\nplot(c0_points_filtered, c1_points_filtered, type = \"l\", col = \"red\", \n     xlab = \"Consommation année 0 (y0)\", ylab = \"Consommation année 1 (y1)\",\n     main = \"Contrainte de Richesse et Courbe d'Indifférence Ajustée\",xlim = c(0,1000),ylim = c(0,1000),lwd=3)\nlines(x_ajuste,y_ajuste,lwd=3,col=\"blue\")\ngrid()"
  },
  {
    "objectID": "posts/index2.html",
    "href": "posts/index2.html",
    "title": "R function that computes all the main descriptive statistics",
    "section": "",
    "text": "This function computeDescriptiveStats takes a numeric vector data as input and computes:\n\nMean\nMedian\nVariance\nStandard Deviation\nSkewness\nKurtosis\nMinimum\nMaximum\nRange (minimum and maximum)\nQuantiles (default quantiles include 0%, 25%, 50%, 75%, 100%)\n\nTo use this function, you need to have R installed on your system, and you should ensure that the e1071 package is installed as well. You can test this function by copying the code into an R script or R console. The set.seed(123) part ensures that the random data generated is the same every time you run the script for reproducibility purposes.\n\nLoad necessary library\n\nif (!require(e1071)) install.packages(\"e1071\")\n\nLoading required package: e1071\n\nlibrary(e1071)\n\n\n\nDefine the function\n\ncomputeDescriptiveStats &lt;- function(data) {\n  if (!is.numeric(data)) {\n    stop(\"Data must be numeric\")\n  }\n  \n  stats &lt;- list(\n    mean = mean(data),\n    median = median(data),\n    variance = var(data),\n    standard_deviation = sd(data),\n    skewness = skewness(data),\n    kurtosis = kurtosis(data),\n    minimum = min(data),\n    maximum = max(data),\n    range = range(data),\n    quantiles = quantile(data)\n  )\n  \n  return(stats)\n}\n\n\n\nTest the function with random data\n\nset.seed(123) # For reproducibility\nrandom_data &lt;- rnorm(100) # Generate 100 random numbers from a normal distribution\n\n\n\nCompute descriptive statistics\n\ndescriptive_stats &lt;- computeDescriptiveStats(random_data)\n\n\n\nPrint the results\n\nprint(descriptive_stats)\n\n$mean\n[1] 0.09040591\n\n$median\n[1] 0.06175631\n\n$variance\n[1] 0.8332328\n\n$standard_deviation\n[1] 0.9128159\n\n$skewness\n[1] 0.05959426\n\n$kurtosis\n[1] -0.217548\n\n$minimum\n[1] -2.309169\n\n$maximum\n[1] 2.187333\n\n$range\n[1] -2.309169  2.187333\n\n$quantiles\n         0%         25%         50%         75%        100% \n-2.30916888 -0.49385424  0.06175631  0.69181917  2.18733299 \n\n\n\n\n\n\nCitationBibTeX citation:@online{boucher,\n  author = {Boucher, Simon-Pierre},\n  title = {R Function That Computes All the Main Descriptive Statistics},\n  url = {https://www.spboucher.net/posts/index2.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoucher, Simon-Pierre. n.d. “R Function That Computes All the Main\nDescriptive Statistics.” https://www.spboucher.net/posts/index2.html."
  },
  {
    "objectID": "posts/index3.html",
    "href": "posts/index3.html",
    "title": "Simple Linear Model",
    "section": "",
    "text": "Simple Linear Model\nThe intuition behind a simple linear regression model is simply to relate two variables having a theoretical economic relationship. The model will allow us to check whether this relation is also reflected empirically. Whether in economics or finance, there are several asset pricing models with a strong theoretical foundation that are not reflected empirically. The best example is the Capital Asset Pricing Model (CAPM), linking the risk premium of the market portfolio with the risk premium of an individual security. The theoretical model can therefore be represented as follows:\n\\[E(R_i)=R_f+\\beta_{i,m} [E(R_m)-R_f]\\]\nWhere :\n\n\\(\\beta_{i,m}\\) represents a measure of the systematic risk of the asset\n\\(E(R_m)\\) represents the expected profitability on the market\n\\(R_f\\) represents the risk free interest rate\n\\(E(R_i)\\) represents the expected profitability on asset \\(i\\)\n\nSince we are going to want to estimate this model empirically, we will use the realized values for the return on asset i, the market and the risk-free rate. The empirical format of the CAPM can therefore be represented as follows:\n\\[(R_i-R_f)=\\alpha+\\beta(R_m-R_f)+\\epsilon\\]\nWhere :\n\n\\((R_i-R_f)\\) represents the dependent variable (we seek to explain the variation of this variable)\n\\((R_m-R_f)\\) represents the independent variable(we will use this variable to explain the independent variable)\n\\(\\alpha\\) is the coefficient estimating the proportion of the excess return on the asset \\(i\\) which is unexplained by the systematic risk\n\\(\\beta\\) is the coefficient estimating the proportion of the excess return on the asset \\(i\\) which is explained by the systematic risk\n\\(\\epsilon\\) represents the error in our model, that is, the variations in return related to idiosyncratic risk.\n\nThis is a specific example of a simple linear regression model and this specification is by no means exhaustive. To have a uniform notation, the simple linear regression model will be represented as follows:\n\\[y_i=\\beta_0+\\beta_1x_i+e_i\\]\nWhere :\n\n\\(y_i\\) represents the dependent variable for the observation \\(i\\)\n\\(x_i\\) represents the independent variable for the observation \\(i\\)\n\\(\\beta_0\\) represents the intercept estimator of our model\n\\(\\beta_1\\) represents the slope estimator of our model\n\\(\\epsilon_i\\) represents the error made by the model\n\nThe error term \\(\\epsilon_i\\) represents the difference between the actual value of our dependent variable and its estimated value. If our model is correctly specified then the error term should not contain any structure linking it to our dependent variable. In other words, the error term \\(\\epsilon_i\\) must be independent and identically distributed random variable with mean zero and constant variance \\(\\sigma^2\\).\n\\[\\epsilon \\sim iid(0,\\sigma^2)\\]\nSince the error term is a random variable then, the dependent variable is also a random variable with the following expectation:\n\\[E(y_i)=E[\\beta_0+\\beta_1x_i+\\epsilon_i]   =E(\\beta_0+\\beta_1x_i)+E(\\epsilon_i)  =\\beta_0+\\beta_1x_i\\]\nAs for the variance of our dependent variable, we find it as follows:\n\\[Var(y_i)=Var[\\beta_0+\\beta_1x_i+\\epsilon_i]   =Var(\\beta_0+\\beta_1x_i)+Var(\\epsilon_i)  =Var(\\epsilon_i)  =\\sigma^2\\]\n\n\n\n\nCitationBibTeX citation:@online{boucher,\n  author = {Boucher, Simon-Pierre},\n  title = {Simple {Linear} {Model}},\n  url = {https://www.spboucher.net/posts/index3.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoucher, Simon-Pierre. n.d. “Simple Linear Model.” https://www.spboucher.net/posts/index3.html."
  },
  {
    "objectID": "posts/index4.html",
    "href": "posts/index4.html",
    "title": "Least squares: two or more independent variables",
    "section": "",
    "text": "In this chapter, we’ll apply the ordinary least-squares method again, but this time if we have more than one independent variable. Here is a regression model with several variables.\n\\[y=\\beta_0+\\beta_1 x_{1i}+\\beta_2 x_{2i}+...+\\beta_N X_{Ni}+\\epsilon\\]\nTo simplify the model, we will now use the matrix format. Here is the change regarding the variables:\n\nLet \\(Y\\) be an \\(n \\times 1\\) vector of observations on the dependent variable.\n\n\\[Y=\\begin{pmatrix} y_1\\\\  y_2\\\\  \\vdots\\\\ y_n\\\\ \\end{pmatrix}\\]\nLet \\(\\beta\\) be an \\(m \\times 1\\) vector of unknown population parameters that we want to estimate.\n\\[\\beta=\\begin{pmatrix} \\beta_1\\\\  \\beta_2\\\\  \\vdots\\\\ \\beta_n\\\\ \\end{pmatrix}\\]\nLet \\(\\epsilon\\) be an \\(n \\times 1\\) vector of disturbances or errors.\n\\[\\epsilon=\\begin{pmatrix} \\epsilon_1\\\\  \\epsilon_2\\\\  \\vdots\\\\ \\epsilon_n\\\\ \\end{pmatrix}\\]\nLet \\(X\\) be an \\(n \\times m\\) matrix where we have observations on \\(m\\) independent variables for \\(n\\) observations.\n\\[X=\\begin{bmatrix}  1&x_{11}  &x_{21}  &\\cdots   &x_{n1} \\\\   1&x_{12}  &x_{22}  & \\cdots &x_{n2} \\\\   \\vdots & \\vdots  & \\vdots  & \\ddots  & \\vdots\\\\   1&x_{1m}  & \\cdots & \\cdots &x_{nm} \\end{bmatrix}\\]\nThis allows us to rewrite the model in a simplified way\n\\[Y=X'\\beta+\\epsilon\\]\nFind the estimator of \\(\\beta\\) by minimizing the sum of squared residuals \\((\\epsilon' \\epsilon)\\)\n\\[\\epsilon'\\epsilon=(Y-X'\\beta)'(Y-X'\\beta)\\]\n\\[\\epsilon'\\epsilon= Y'Y-\\beta'X'Y-Y'X \\beta + \\beta'X'X \\beta\\]\n\\[\\epsilon'\\epsilon= Y'Y -2 \\beta' X'Y+ \\beta'X'X \\beta\\]\nThe partial derivatives with respect to \\(\\beta\\) is solved as follows :\n\\[\\frac{\\partial \\epsilon'\\epsilon}{\\partial \\beta}= 0\\]\nIt is now possible for us to find the value of \\(\\beta\\):\n\\[-2X'Y+2X'X \\beta=0\\]\n\\[X'Y=X'X \\beta\\]\nThe solution for the estimator of \\(\\beta\\):\n\\[\\hat{\\beta}=(X'X)^{-1}X'Y\\]\n\n\n\nCitationBibTeX citation:@online{boucher,\n  author = {Boucher, Simon-Pierre},\n  title = {Least Squares: Two or More Independent Variables},\n  url = {https://www.spboucher.net/posts/index4.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoucher, Simon-Pierre. n.d. “Least Squares: Two or More\nIndependent Variables.” https://www.spboucher.net/posts/index4.html."
  },
  {
    "objectID": "posts/index5.html",
    "href": "posts/index5.html",
    "title": "Least squares estimation",
    "section": "",
    "text": "In this chapter we present a way to obtain a value for our coefficient of the simple linear regression model. We start with the ordinary least squares estimation method. This value will be found by minimizing the sum of the squared errors of our regression. We can express the error term as follows :\n\\[\\epsilon_i=y_i-\\beta_0+\\beta_1x_i\\]\nThen we perform a summation for all the observations of our squared error term going from \\(1\\) to \\(n\\), i.e. the sum of the squared errors \\(S(\\beta_0,\\beta_1)\\).\n\\[S(\\beta_0,\\beta_1)=\\sum_{i=1}^n\\epsilon_i^2=\\sum_{i=1}^n(y_i-\\beta_0+\\beta_1 x_i)^2\\]\nThen we minimize \\(S(\\beta_0,\\beta_1)\\) with respect to \\(\\beta_0\\) and \\(\\beta_1\\). The partial derivatives with respect to \\(\\beta_0\\) is solved as follows.\n\\[\\frac{\\partial S(\\beta_0,\\beta_1)}{\\partial \\beta_0}=-2\\sum_{i=1}^n(y_i-\\beta_0-\\beta_1 x_i)\\]\nThe partial derivatives with respect to \\(\\beta_1\\) is solved as follows.\n\\[\\frac{\\partial S(\\beta_0,\\beta_1)}{\\partial \\beta_1}=-2\\sum_{i=1}^n(y_i-\\beta_0-\\beta_1 x_i)x_i\\]\nThe first order condition makes it possible to equalize our two partial derivative at 0 and we can thus find the solution.\n\\[\\frac{\\partial S(\\beta_0,\\beta_1)}{\\partial \\beta_0}=0\\]\n\\[\\frac{\\partial S(\\beta_0,\\beta_1)}{\\partial \\beta_1}=0\\]\nThe solution for the \\(\\beta_0\\) estimator is as follows:\n\\[-2\\sum_{i=1}^n(y_i-\\beta_0-\\beta_1 x_i)=0\\]\nWe can set the following properties :\n\n\\(\\sum_{i=1}^ny_i=n \\overline{y}\\)\n\\(\\sum_{i=1}^n \\beta_0=n \\beta_0\\)\n\\(\\sum_{i=1}^n \\beta_1 x_i=n \\beta_1 \\overline{x}\\)\n\n\\[n\\overline{y}-n \\beta_0-n \\beta_1 \\overline{x}=0\\]\n\\[\\beta_0=\\frac{n\\overline{y}-n \\beta_1 \\overline{x}}{n}\\]\nSolution for the estimator of \\(\\beta_0\\) :\n\\[\\hat{\\beta_0}=\\overline{y}-\\beta_1 \\overline{x}\\]\nThe solution for the \\(\\beta_1\\) estimator is as follows:\n\\[-2\\sum_{i=1}^n(y_i-\\beta_0-\\beta_1 x_i)x_i=0\\]\n\\[-2\\sum_{i=1}^n(y_i-(\\overline{y}-\\beta_1 \\overline{x})-\\beta_1 x_i)x_i=0\\]\n\\[\\sum_{i=1}^nx_iy_i-n \\overline{y} \\overline{x}+n \\beta_1 \\overline{x}^2-\\beta_1\\sum_{i=1}^nx_i^2=0\\]\n\\[\\sum_{i=1}^nx_iy_i-n \\overline{y} \\overline{x}=\\beta_1\\sum_{i=1}^nx_i^2-n \\beta_1 \\overline{x}^2\\]\n\\[\\sum_{i=1}^nx_iy_i-n \\overline{y} \\overline{x}=\\beta_1\\left(\\sum_{i=1}^nx_i^2-n \\overline{x}^2\\right)\\]\n\\[\\beta_1=\\frac{\\sum_{i=1}^nx_iy_i-n \\overline{y} \\overline{x}}{\\sum_{i=1}^nx_i^2-n \\overline{x}^2}\\]\nWe can set the following properties :\n\n\\(\\sum_{i=1}^nx_iy_i-n \\overline{y} \\overline{x}=(x_i-\\overline{x})(y_i-\\overline{y})\\)\n\\(\\sum_{i=1}^nx_i^2-n \\overline{x}^2=(x_i-\\overline{x})^2\\)\n\nSolution for the estimator of \\(\\beta_1\\) :\n\\[\\hat{\\beta_1}=\\frac{(x_i-\\overline{x})(y_i-\\overline{y})}{(x_i-\\overline{x})^2}\\]\n\n\n\nCitationBibTeX citation:@online{boucher,\n  author = {Boucher, Simon-Pierre},\n  title = {Least Squares Estimation},\n  url = {https://www.spboucher.net/posts/index5.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoucher, Simon-Pierre. n.d. “Least Squares Estimation.” https://www.spboucher.net/posts/index5.html."
  }
]